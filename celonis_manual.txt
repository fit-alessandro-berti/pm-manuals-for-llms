Celonis is a leading platform for process mining. The main concepts that you should know about Celonis are:
- Data Pool: a relational database (in particular, Vertica SQL is used), usually without defining the foreign keys between tables
- Data Model: an abstraction built on the data pool, in which the foreign keys between tables are defined. A data pool can contain different data models. A data model can contain different process configurations.
- A process configuration defines an activity table (containing the events and their attributes), expliciting which attributes should be considered as activity, timestamp and case identifier. Optionally, it is also possible to define a case table, containing the case identifier (same attribute name as in the activity table) and the attributes at the case level.
- The data model should be seen as a snowflake schema. The activity table should be connected to the case table via a foreign key. Additional information about the attributes at the case level (for example, details about a customer or a supplier) can be stored in different tables. The case table should be connected to these tables via a foreign key.
- Once defined the data model, including the different process configurations, there is a loading of the data model to the SAOLA in-memory columnar database. The entire tables are loaded in-memory.
- Once loaded, packages (collections of analyses) can be defined on top of a data model. Packages can be grouped in spaces. The access permissions (defining who can read/write the packages) are managed at the space level.
- It is possible to define additional domain knowledge about a package inside the so-called knowledge models. A package can contain different knowledge models.

The Pycelonis library can be used to connect to the Celonis platform.
The Pycelonis library can be installed from PIP using the command:
pip install --extra-index-url=https://pypi.celonis.cloud/ -U pycelonis pycelonis-core

To connect to Celonis, the URL of the instance and an API token (which can be obtained clicking the icon at the bottom left of the Celonis interface, going to "Edit Profile", going to the "API keys" section, putting a name under "New API Key Name" and clicking "Create API Key") are needed.
A key type needs to be provided between USER_KEY (most common option) or APP_KEY

import pycelonis
celonis = pycelonis.get_celonis("https://celonis-instance.eu-1.celonis.cloud", api_token="OBTAINED_TOKEN", key_type="USER_KEY")

After connection, the "celonis" object allow to access the "data_integration" (definition of data pool and data models, and operations over them) and "studio" (allowing to define spaces and packages).

The data integration component can be accessed using:

celonis.data_integration

The studio component can be accessed using:

celonis.studio


To create a data pool, the following instruction can be used:

data_pool = celonis.data_integration.create_data_pool("name of the data pool")

To retrieve an existing data pool starting from its name, the following instruction can be used:

data_pool = data_pool.data_integration.get_data_pools().find("name of the data pool")

Tables can be pushed to the data pool starting from Pandas dataframes objects. For example, the following instructions can be used to push a table contained in a CSV to a data pool:

import pandas as pd

dataframe = pd.read_csv("name_of_file.csv", sep=",")
data_pool.create_table(dataframe, "name of the table")

The create_table function accepts a Pandas dataframe and the name of the table. Additional parameters can be provided. For example, if a table with the same name is contained in the data pool and we want to replace that without exceptions, the following instruction can be ued:

data_pool.create_table(dataframe, "name of the table", force=True, drop_if_exists=True)


Data models can be managed starting from the data pool object. In particular, to create a data model, the following instruction can be used:

data_model = data_pool.create_data_model("name of the data model")

To retrieve an existing data model, the following instruction can be used:

data_model = data_pool.get_data_models().find("name of the data model")

To add a table from the data pool to the data model, the following instruction can be used:

tab = data_model.add_table("name of the table in the data pool", "alias of the table in the data model")

the alias of the table in the data model is usually set to be identical to the name of the table in the data pool.
The returned "tab" object is important as it contains the identifier of the table in the data model (tab.id) that is essential to define foreign keys and process configurations.
Foreign keys between two tables can be defined as follow, assuming that they link on the "fkey" column.

tab1 = data_model.add_table("name of the table 1 in the data pool", "alias of the table 2 in the data model")
tab2 = data_model.add_table("name of the table 2 in the data pool", "alias of the table 2 in the data model")

data_model.create_foreign_key(tab1.id, tab2.id, columns=[("fkey", "fkey")])

If the foreign key is composed by two columns, then the "columns" argument will consist of two tuples.


Eventually, when the tables are inserted and the foreign keys created, a process configuration can be created on the data model using the command:

data_model.create_process_configuration(activity_table_id=tab1.id, case_table_id=tab2.id, case_id_column="Case ID column", activity_column="Activity Column", timestamp_column="Timestamp Column")

The last step is about reloading the data model. This can be done using the command:

data_model.reload()

It is also possible to delete data pools and data models, using the functions   data_pool.delete()   and    data_model.delete()    respectively

To retrieve the different process configurations associated to a data model, the function    data_model.get_process_configurations()    can be used.
The output is a collection of process configurations. Each process configuration has some properties:
* activity_table_id:  the identifier of the activity table.
* case_table_id:      the identifier of the case table (if provided, it may be None).
* case_id_column:     the case ID column that is used (from the activity and the case tables)
* activity_column:    the activity column (from the activity table)
* timestamp_column:   the timestamp column (from the timestamp table)

The first process configuration (in order) is the most important as it is by default used in the Celonis analytics.

Therefore, it is very important to keep track of the correspondence between the tables in the data models and the identifiers.
The list of tables (containing minimal information) can be retrieved using the function    data_model.get_tables()
For every table, at least tab.id   and    tab.name   are already retrieved by the get_tables() function.
Iterating over this list, more attributes over a table can be retrieved using the function    table = data_model.get_table(tab.id)
This includes the list of columns (table.get_columns()) of the table.

Data models can be queried using the PQL querying language. PQL is a language working on the SAOLA in-memory columnar engine.
The main goal of PQL is to produce a table as output. Each request can be annotated with some filters, limiting the considered data.

The needed imports are:

from pycelonis.pql.pql import PQL, PQLColumn, PQLFilter


A PQL object needs to be defined:

pql = PQL()


As many PQLColumn objects as many columns are desidered in the output table need to be defined and added to the "pql" object. Each column has its own PQL query.

pql.add(PQLColumn(name="unique name of the column in the output table", query="PQL query 1"))
pql.add(PQLColumn(name="unique name of the column in the output table", query="PQL query 2"))


The filters can be added as PQLFilter objects

pql.add(PQLFilter(query='PQL query 3'))
pql.add(PQLFilter(query='PQL query 4'))

A simple example, retrieving the case identifier, activity and timestamp colum from the activity table called "Events" is provided:

pql = PQL()
pql.add(PQLColumn(name="Case Identifier", query="\"Events\".\"CaseID\""))
pql.add(PQLColumn(name="Activity", query="\"Events\".\"Activity\""))
pql.add(PQLColumn(name="Timestamp", query="\"Events\".\"Timestamp\""))


The output will have as many rows as the events table.

To apply a filter on the rows having "Confirmation of Receipt" as activity, and keep the same columns, a PQLFilter can be added, for example:

pql = PQL()
pql.add(PQLColumn(name="Case Identifier", query="\"Events\".\"CaseID\""))
pql.add(PQLColumn(name="Activity", query="\"Events\".\"Activity\""))
pql.add(PQLColumn(name="Timestamp", query="\"Events\".\"Timestamp\""))
pql.add(PQLFilter(query="FILTER \"Events\".\"Activity\" = 'Confirmation of Receipt'"))

The PQL query can be then executed, retrieving a Pandas dataframe, using:

dataframe = data_model.export_data_frame(pql)


On a note, the filter cannot be applied on top of the resulting column names, but need to take into account queries to the original table(s).

The following comparison operators are available in PQL:

A > B   (greater than)
A >= B  (greater or equal than)
A < B   (lower than)
A <= B   (lower or equal than)
A + B    sum
A - B    difference
A * B    multiplication
A / B    division
A OR B
A AND B
A = B
A != B   true if A is not equal to B

to verify if a value is null: ISNULL(A) = 1
to verify if a value is not null:  ISNULL(A) = 0


An important operator is CASE_WHEN, which evaluates a set of conditions.
Examples:

CASE WHEN \"Events\".\"Activity\" = 'Trial' THEN 1 ELSE 0 END

CASE WHEN \"Events\".\"Activity\" = 'Trial' THEN 1 WHEN \"Events\".\"Activity\" = 'Confirm' THEN 2 ELSE 0 END

An important operator is REMAP_VALUES, which allows to re-map values of STRING columns.
For example,

REMAP_VALUES( \"Events\".\"Activity\", ['X', 'TRIAL'], ['Y', NULL], ['Z', NULL])

Changes the activity column in the following way:
* the activity 'X' is remapped to 'TRIAL'
* the activity 'Y' is mapped to null, so it disappears
* the activity 'Z' is mapped to null, so it disappears
* any other activity is unchanged

With:
REMAP_VALUES( \"Events\".\"Activity\", ['X', 'TRIAL'], ['Y', NULL], ['Z', NULL], NULL)
We map any other activity to null, so any other activity disappears.

PQL reduces the inherent complexity for the user to define PQL queries, so joins are resolved automatically.

For example, if we want to filter on an attribute at the case level (supposing "Cases" is the case table), the filter would look as follow:

pql.add(PQLFilter(query="FILTER \"Cases\".\"Quantity\" >= 500"))

When this filter is added, only the events belonging to the cases for which the quantity (defined for instance in the case table) is greater or equal than 500 are kept in the output.

Grouping operators are also applied automatically. For example:

pql = PQL()
pql.add(PQLColumn(name="Activity", query="\"Events\".\"Activity\""))
pql.add(PQLColumn(name="Total Count", query="COUNT(\"Events\".\"Activity\")"))

Provides a table in which each activity is associated with the total number of occurrences in the events table. Another example:

pql = PQL()
pql.add(PQLColumn(name="Activity", query="DISTINCT \"Events\".\"Activity\""))
pql.add(PQLColumn(name="Distinct Cases Count", query="COUNT(DISTINCT \"Events\".\"Activity\")"))

Here, the distinct operator is applied first, keeping for each case only a single occurrence of the activity.
The count command is then applied to this intermediate results, producing the number of distinct cases for which each activity occurs.

The VARIANT operator concatenates all the values for a particular attribute inside a case (process variant). So it returns a single string for each case.
The following example shows how to know the process variant related to each case:

pql = PQL()
pql.add(PQLColumn(name="Case Identifier", query="\"Events\".\"CaseID\""))
pql.add(PQLColumn(name="Process Variant", query="VARIANT(\"Events\".\"Activity\")"))

If we want to know how much frequent is each process variant, the COUNT operator can be then applied:

pql = PQL()
pql.add(PQLColumn(name="Process Variant", query="VARIANT(\"Events\".\"Activity\")"))
pql.add(PQLColumn(name="Variant Occurrences", query="COUNT(VARIANT(\"Events\".\"Activity\"))"))

Other aggregation operators:

AVG(\"Table\".\"Column\")    computes the mean
FIRST(\"Table\".\"Column\")  gets the first element
LAST(\"Table\".\"Column\")   gets the last element
MAX(\"Table\".\"Column\")    gets the maximum value
MEDIAN(\"Table\".\"Column\")  gets the median value
MIN(\"Table\".\"Column\")    gets the minimum value
MODE(\"Table\".\"Column\")   gets the most frequent value
PRODUCT(\"Table\".\"Column\")  multiplies all the values
QUANTILE(\"Table\".\"Column\", quantile)   gets a quantile
STDEV(\"Table\".\"Column\")  computes the standard deviation
STRING_AGG(\"Table\".\"Column\", delimiter)   aggregates the strings with the provided delimiter (but is slower than the VARIANT operator)
SUM(\"Table\".\"Column\")     computes the sum of the values of a column


The SOURCE and TARGET operators combine values of different rows of the activity table depending on the specified edge configuration.

Possible edge configurations:

* ANY_OCCURRENCE[] TO ANY_OCCURRENCE[]    (directly-follows graph)
* FIRST_OCCURRENCE[] TO ANY_OCCURRENCE[]   (connects the first event of a case to all the other events of the same case)
* FIRST_OCCURRENCE[] TO ANY_OCCURRENCE_WITH_SELF[]  (connects the first event of a case with all the events of the same case, including himself)
* ANY_OCCURRENCE[] TO LAST_OCCURRENCE[]    (connects all the events of a case except the last, to the last event of the case)
* FIRST_OCCURRENCE[] TO LAST_OCCURRENCE[]    (connects the first event of a case to the last event of a case)

For each PQL object, if the SOURCE/TARGET operators are used, only one edge configuration is possible (not more than one).

Let's see an example related to the computation of the performance directly-follows graph.
We will make usage of the operator SECONDS_BETWEEN which counts the seconds between two date values.
If we want to compute the difference in timestamps, but keep the reference to the case identifier, we could use the code:

pql = PQL()
pql.add(PQLColumn(name="Case Identifier", query="\"Events\".\"CaseID\""))
pql.add(PQLColumn(name="Source Activity", query="SOURCE(\"Events\".\"Activity\", ANY_OCCURRENCE[] TO ANY_OCCURRENCE[])"))
pql.add(PQLColumn(name="Target Activity", query="TARGET(\"Events\".\"Activity\")"))
pql.add(PQLColumn(name="Time Between", query="SECONDS_BETWEEN(SOURCE(\"Events\".\"Timestamp\"), TARGET(\"Events\".\"Timestamp\"))"))

An example filtering on the TARGET operator (keeping only arcs going into the 'X' activity) follows:

pql = PQL()
pql.add(PQLColumn(name="Case Identifier", query="\"Events\".\"CaseID\""))
pql.add(PQLColumn(name="Source Activity", query="SOURCE(\"Events\".\"Activity\", ANY_OCCURRENCE[] TO ANY_OCCURRENCE[])"))
pql.add(PQLColumn(name="Target Activity", query="TARGET(\"Events\".\"Activity\", REMAP(\"Events\".\"Activity\", ['X', 'X'], null))"))


Both the SOURCE and TARGET operators accept, before the edge configuration, a possible PQL filter (for example, filtering the timestamp) could be applied.

PQL also provides pull-up functions which aggregate columns based on other tables.
A typical example is querying the case table, while aggregating on the activity table.
The available operators follow:

PU_AVG(\"Case Table\", \"Activity Table\".\"Column\")    computes the mean
PU_FIRST(\"Case Table\", \"Activity Table\".\"Column\")  gets the first element
PU_LAST(\"Case Table\", \"Activity Table\".\"Column\")   gets the last element
PU_MAX(\"Case Table\", \"Activity Table\".\"Column\")    gets the maximum value
PU_MEDIAN(\"Case Table\", \"Activity Table\".\"Column\")  gets the median value
PU_MIN(\"Case Table\", \"Activity Table\".\"Column\")    gets the minimum value
PU_MODE(\"Case Table\", \"Activity Table\".\"Column\")   gets the most frequent value
PU_PRODUCT(\"Case Table\", \"Activity Table\".\"Column\")  multiplies all the values
PU_QUANTILE(\"Case Table\", \"Activity Table\".\"Column\", quantile)   gets a quantile
PU_STDEV(\"Case Table\", \"Activity Table\".\"Column\")  computes the standard deviation
PU_STRING_AGG(\"Case Table\", \"Activity Table\".\"Column\", delimiter)   aggregates the strings with the provided delimiter (but is slower than the VARIANT operator)
PU_SUM(\"Case Table\", \"Activity Table\".\"Column\")     computes the sum of the values of a column

For example, if you want to associate each case with the corresponding start activities, the following code can be used:

pql = PQL()
pql.add(PQLColumn(name="Case Identifier", query="\"Cases\".\"CaseID\""))
pql.add(PQLColumn(name="Start Activity", query="PU_FIRST(\"Cases\", \"Events\".\"Activity\"")))

If you want to associate each start activity with the corresponding number of occurrences in the log (as first activity), the following code can be used:

pql = PQL()
pql.add(PQLColumn(name="Start Activity", query="PU_FIRST(\"Cases\", \"Events\".\"Activity\"")))
pql.add(PQLColumn(name="Occurrences", query="COUNT(PU_FIRST(\"Cases\", \"Events\".\"Activity\""))))

Note that the pull-up aggregation requires two distinct tables to be applied, and cannot be applied on a single table.
Usually, it is applied on the case table, aggregating on the activity table.

Celonis offers moving aggregation operators.
The general syntax is:
MOVING_X(\"Table\".\"Column\", start, end)

in which MOVING_X can be: MOVING_AVG, MOVING_COUNT_DISTINCT, MOVING_COUNT, MOVING_MAX, MOVING_MEDIAN, MOVING_MIN, MOVING_STDEV, MOVING_SUM, MOVED_TRIMMED_MEAN, MOVING_VAR

start is the start of the window relative to the current row (negative or positive integer)
end is the end of the window relative to the current row (negative or positive integer)
Note that the maximum difference between "end" and "start" is 1000 (does not work otherwise).
So, -500 to 500, or 0 to 1000 should be a safe choice.

RUNNING_SUM (or alternatively RUNNING_TOTAL) returns the sum of all the previous rows.
The RUNNING_SUM operator accepts also an order on the rows, for example:

RUNNING_SUM(\"Events\".\"Cost\", ORDER BY ( \"Events\".\"Timestamp\" ))

The RUNNING_SUM accepts also a partitioning, example

RUNNING_SUM ( "Table1"."column" , PARTITION BY ( "Table1"."Country" ) )



WINDOW_AVG computes the average over a user-defined window.
Examples:

WINDOW_AVG ( "Table"."values" , -1 , 1 , PARTITION BY ( "Table"."code" ) )

Computes the average of the previous, current, and next row, dividing the data into partitions.

Note that the maximum difference between "end" and "start" is 1000 (does not work otherwise).
So, -500 to 500, or 0 to 1000 should be a safe choice.


Other process operators:

ACTIVATION_COUNT(\"Activity Table\".\"Column\")   tells how many times an activity has already occurred
ACTIVIY_LAG(\"Activity Table\".\"Column\", offset)    returns the value for the row preceding (by offset) the current row
ACTIVIY_LEAD(\"Activity Table\".\"Column\", offset)   returns the value for the row succeedings (by offset) the current row
INDEX_ACTIVITY_LOOP(\"Activity Table\".\"Column\")    returns how many times every activity has occurred in direct succession
INDEX_ACTIVITY_ORDER(\"Activity Table\".\"Column\")   returns the position of each activity in the case
INDEX_ACTIVITY_LOOP_REVERSE(\"Activity Table\".\"Column\")    returns how many times every activity has occurred in direct succession in reverse order
INDEX_ACTIVITY_ORDER_REVERSE(\"Activity Table\".\"Column\")   returns the position of each activity in the case in reverse order

Example computation of work-in-progress at every point in time of a process (counting how many cases are open at the specific point in time):

pql = PQL()
pql.add(PQLColumn(name="Case Identifier", query="\"Events\".\"CaseID\""))
pql.add(PQLColumn(name="Activity", query="\"Events\".\"Activity\""))
pql.add(PQLColumn(name="Timestamp", query="\"Events\".\"Timestamp\""))
pql.add(PQLColumn(name="Work in Progress", query="RUNNING_SUM( CASE WHEN INDEX_ACTIVITY_ORDER ( \"Events\".\"Activity\" ) = 1 THEN 1 WHEN INDEX_ACTIVITY_ORDER_REVERSE ( \"Events\".\"Activity\" ) = 1 THEN -1 ELSE 0 END, ORDER BY ( \"Events\".\"Timestamp\" ) )"))


OBJECT-CENTRIC DATA MODELS

Celonis offer support for object-centric process mining, in which an event might be related to different objects to different object types.
The types of data models used for object-centric event data is different than traditional process configurations.

First, every non-alphanumeric and space character should be stripped out from the name of the activity/object type.

The naming convention is strict. It starts from arbitrarily choosing a namespace (only admitted values: “celonis”, “custom”) and creating:
-	A table "e_<namespace>_<activityname>" for each distinct event type/activity. It should contain at least the columns ID (the event identifier, that can be a self-incremental number only if there is no other specification), Timestamp (also here the names are strict!), and Type (the activity) are required. If there are two activities, let's say "A" and "B", there should be two tables, "e_<namespace>_A" and "e_<namespace"_B".
-	A table "o_<namespace>_<objtype>" for each distinct object type. It should contain at least the column ID (the object identifier, which should be a string) and Type (the object type). If there are two object types, let's say "order" and "item", there should be two tables, "o_<namespace>_order" and "o_<namespace>_item".
-	A table "r_e_<namespace>_<activityname>_<objtype>" for each distinct (event type, object type) relationship (E2O relationships). They should contain at least the columns EventID and ObjectID.

To retrieve the list of activities, object types, and event-to-object relationships, you need to download the list of tables of the data model and check whether they start respectively with "e_" (activities), "o_" (object types), "r_e_" (relationships)

The CREATE_EVENTLOG flattens an object-centric data model under an object type to a traditional event log.

Examples:

CREATE_EVENTLOG("o_celonis_SalesOrderItem",INCLUDE["e_celonis_CreateDeliveryItem"]) creates a traditional event log, flattened on the "o_celonis_SalesOrderItem" object type, where all the traditional PQL operators can be applied.
The flattened log, in this case, includes just the CreateDeliveryItem activity. If the INCLUDE attribute is not specified, then all the activities related to the object type are considered.

Available fields: LEAD_OBJECT_ID, ACTIVITY, TIMESTAMP

CREATE_EVENTLOG("o_celonis_SalesOrderItem",INCLUDE["e_celonis_CreateDeliveryItem"])."LEAD_OBJECT_ID"
CREATE_EVENTLOG("o_celonis_SalesOrderItem",INCLUDE["e_celonis_CreateDeliveryItem"])."ACTIVITY"
CREATE_EVENTLOG("o_celonis_SalesOrderItem",INCLUDE["e_celonis_CreateDeliveryItem"])."TIMESTAMP"

If you want to compute the average time between two activities, use the SOURCE-TARGET operators

* Filter on SOURCE("table"."activityfield") == 'firstactivity' (in the flattened log: 'e_custom_firstactivity')
* Filter on TARGET("table"."activityfield") == 'secondactivity' (in the flattened log: 'e_custom_targetactivity')
* Measure the average time
AVG(SECONDS_BETWEEN(SOURCE("table"."timestampfield"), TARGET("table"."timestampfield")))
