Celonis is a leading platform for process mining. The main concepts that you should know about Celonis are:
- Data Pool: a relational database (in particular, Vertica SQL is used), usually without defining the foreign keys between tables
- Data Model: an abstraction built on the data pool, in which the foreign keys between tables are defined. A data pool can contain different data models. A data model can contain different process configurations.
- A process configuration defines an activity table (containing the events and their attributes), expliciting which attributes should be considered as activity, timestamp and case identifier. Optionally, it is also possible to define a case table, containing the case identifier (same attribute name as in the activity table) and the attributes at the case level.
- The data model should be seen as a snowflake schema. The activity table should be connected to the case table via a foreign key. Additional information about the attributes at the case level (for example, details about a customer or a supplier) can be stored in different tables. The case table should be connected to these tables via a foreign key.
- Once defined the data model, including the different process configurations, there is a loading of the data model to the SAOLA in-memory columnar database. The entire tables are loaded in-memory.
- Once loaded, packages (collections of analyses) can be defined on top of a data model. Packages can be grouped in spaces. The access permissions (defining who can read/write the packages) are managed at the space level.
- It is possible to define additional domain knowledge about a package inside the so-called knowledge models. A package can contain different knowledge models.

The Pycelonis library can be used to connect to the Celonis platform.
The Pycelonis library can be installed from PIP using the command:
pip install --extra-index-url=https://pypi.celonis.cloud/ -U pycelonis pycelonis-core

To connect to Celonis, the URL of the instance and an API token (which can be obtained clicking the icon at the bottom left of the Celonis interface, going to "Edit Profile", going to the "API keys" section, putting a name under "New API Key Name" and clicking "Create API Key") are needed.
A key type needs to be provided between USER_KEY (most common option) or APP_KEY

import pycelonis
celonis = pycelonis.get_celonis("https://celonis-instance.eu-1.celonis.cloud", api_token="OBTAINED_TOKEN", key_type="USER_KEY")

After connection, the "celonis" object allow to access the "data_integration" (definition of data pool and data models, and operations over them) and "studio" (allowing to define spaces and packages).

The data integration component can be accessed using:

celonis.data_integration

The studio component can be accessed using:

celonis.studio


To create a data pool, the following instruction can be used:

data_pool = celonis.data_integration.create_data_pool("name of the data pool")

To retrieve an existing data pool starting from its name, the following instruction can be used:

data_pool = data_pool.data_integration.get_data_pools().find("name of the data pool")

Tables can be pushed to the data pool starting from Pandas dataframes objects. For example, the following instructions can be used to push a table contained in a CSV to a data pool:

import pandas as pd

dataframe = pd.read_csv("name_of_file.csv", sep=",")
data_pool.create_table(dataframe, "name of the table")

The create_table function accepts a Pandas dataframe and the name of the table. Additional parameters can be provided. For example, if a table with the same name is contained in the data pool and we want to replace that without exceptions, the following instruction can be ued:

data_pool.create_table(dataframe, "name of the table", force=True, drop_if_exists=True)


Data models can be managed starting from the data pool object. In particular, to create a data model, the following instruction can be used:

data_model = data_pool.create_data_model("name of the data model")

To retrieve an existing data model, the following instruction can be used:

data_model = data_pool.get_data_models().find("name of the data model")

To add a table from the data pool to the data model, the following instruction can be used:

tab = data_model.add_table("name of the table in the data pool", "alias of the table in the data model")

the alias of the table in the data model is usually set to be identical to the name of the table in the data pool.
The returned "tab" object is important as it contains the identifier of the table in the data model (tab.id) that is essential to define foreign keys and process configurations.
Foreign keys between two tables can be defined as follow, assuming that they link on the "fkey" column.

tab1 = data_model.add_table("name of the table 1 in the data pool", "alias of the table 2 in the data model")
tab2 = data_model.add_table("name of the table 2 in the data pool", "alias of the table 2 in the data model")

data_model.create_foreign_key(tab1.id, tab2.id, columns=[("fkey", "fkey")])

If the foreign key is composed by two columns, then the "columns" argument will consist of two tuples.


Eventually, when the tables are inserted and the foreign keys created, a process configuration can be created on the data model using the command:

data_model.create_process_configuration(activity_table_id=tab1.id, case_table_id=tab2.id, case_id_column="Case ID column", activity_column="Activity Column", timestamp_column="Timestamp Column")

The last step is about reloading the data model. This can be done using the command:

data_model.reload()

It is also possible to delete data pools and data models, using the functions   data_pool.delete()   and    data_model.delete()    respectively

Data models can be queried using the PQL querying language. PQL is a language working on the SAOLA in-memory columnar engine.
The main goal of PQL is to produce a table as output. Each request can be annotated with some filters, limiting the considered data.

The needed imports are:

from pycelonis.pql.pql import PQL, PQLColumn, PQLFilter


A PQL object needs to be defined:

pql = PQL()


As many PQLColumn objects as many columns are desidered in the output table need to be defined and added to the "pql" object. Each column has its own PQL query.

pql.add(PQLColumn(name="unique name of the column in the output table", query="PQL query 1"))
pql.add(PQLColumn(name="unique name of the column in the output table", query="PQL query 2"))


The filters can be added as PQLFilter objects

pql.add(PQLFilter(query='PQL query 3'))
pql.add(PQLFilter(query='PQL query 4'))

A simple example, retrieving the case identifier, activity and timestamp colum from the activity table called "Events" is provided:

pql = PQL()
pql.add(PQLColumn(name="Case Identifier", query="\"Events\".\"CaseID\""))
pql.add(PQLColumn(name="Activity", query="\"Events\".\"Activity\""))
pql.add(PQLColumn(name="Timestamp", query="\"Events\".\"Timestamp\""))


The output will have as many rows as the events table.

To apply a filter on the rows having "Confirmation of Receipt" as activity, and keep the same columns, a PQLFilter can be added, for example:

pql = PQL()
pql.add(PQLColumn(name="Case Identifier", query="\"Events\".\"CaseID\""))
pql.add(PQLColumn(name="Activity", query="\"Events\".\"Activity\""))
pql.add(PQLColumn(name="Timestamp", query="\"Events\".\"Timestamp\""))
pql.add(PQLFilter(query="\"Events\".\"Activity\" = 'Confirmation of Receipt'"))

The following comparison operators are available in PQL:

A > B   (greater than)
A >= B  (greater or equal than)
A < B   (lower than)
A <= B   (lower or equal than)
A + B    sum
A - B    difference
A * B    multiplication
A / B    division
A OR B
A AND B
A EQUAL B
A NOT_EQ_TO   true if A is not equal to B

PQL reduces the inherent complexity for the user to define PQL queries, so joins are resolved automatically.

For example, if we want to filter on an attribute at the case level (supposing "Cases" is the case table), the filter would look as follow:

pql.add(PQLFilter(query="\"Cases\".\"Quantity\" >= 500"))

When this filter is added, only the events belonging to the cases for which the quantity (defined for instance in the case table) is greater or equal than 500 are kept in the output.

Grouping operators are also applied automatically. For example:

pql = PQL()
pql.add(PQLColumn(name="Activity", query="\"Events\".\"Activity\""))
pql.add(PQLColumn(name="Total Count", query="COUNT(\"Events\".\"Activity\")"))

Provides a table in which each activity is associated with the total number of occurrences in the events table. Another example:

pql = PQL()
pql.add(PQLColumn(name="Activity", query="DISTINCT \"Events\".\"Activity\""))
pql.add(PQLColumn(name="Distinct Cases Count", query="COUNT(DISTINCT \"Events\".\"Activity\")"))

Here, the distinct operator is applied first, keeping for each case only a single occurrence of the activity.
The count command is then applied to this intermediate results, producing the number of distinct cases for which each activity occurs.

The VARIANT operator concatenates all the values for a particular attribute inside a case (process variant). So it returns a single string for each case.
The following example shows how to know the process variant related to each case:

pql = PQL()
pql.add(PQLColumn(name="Case Identifier", query="\"Events\".\"CaseID\""))
pql.add(PQLColumn(name="Process Variant", query="VARIANT(\"Events\".\"Activity\")"))

If we want to know how much frequent is each process variant, the COUNT operator can be then applied:

pql = PQL()
pql.add(PQLColumn(name="Process Variant", query="VARIANT(\"Events\".\"Activity\")"))
pql.add(PQLColumn(name="Variant Occurrences", query="COUNT(VARIANT(\"Events\".\"Activity\"))"))


The SOURCE and TARGET operators combine values of different rows of the activity table depending on the specified edge configuration.

Possible edge configurations:

* ANY_OCCURRENCE[] TO ANY_OCCURRENCE[]    (directly-follows graph)
* FIRST_OCCURRENCE[] TO ANY_OCCURRENCE[]   (connects the first event of a case to all the other events of the same case)
* FIRST_OCCURRENCE[] TO ANY_OCCURRENCE_WITH_SELF[]  (connects the first event of a case with all the events of the same case, including himself)
* ANY_OCCURRENCE[] TO LAST_OCCURRENCE[]    (connects all the events of a case except the last, to the last event of the case)
* FIRST_OCCURRENCE[] TO LAST_OCCURRENCE[]    (connects the first event of a case to the last event of a case)

For each PQL object, if the SOURCE/TARGET operators are used, only one edge configuration is possible (not more than one).

Let's see an example related to the computation of the performance directly-follows graph.
We will make usage of the operator SECONDS_BETWEEN which counts the seconds between two date values.
If we want to compute the difference in timestamps, but keep the reference to the case identifier, we could use the code:

pql = PQL()
pql.add(PQLColumn(name="Case Identifier", query="\"Events\".\"CaseID\""))
pql.add(PQLColumn(name="Source Activity", query="SOURCE(\"Events\".\"Activity\", ANY_OCCURRENCE[] TO ANY_OCCURRENCE[])"))
pql.add(PQLColumn(name="Target Activity", query="TARGET(\"Events\".\"Activity\")"))
pql.add(PQLColumn(name="Time Between", query="SECONDS_BETWEEN(SOURCE(\"Events\".\"Timestamp\"), TARGET(\"Events\".\"Timestamp\"))"))


Both the SOURCE and TARGET operators accept, before the edge configuration, a possible PQL filter (for example, filtering the timestamp) could be applied.

