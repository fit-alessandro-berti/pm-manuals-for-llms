TRADITIONAL EVENT LOGS

In pm4py, the preferred structure for storing traditional event logs are the Pandas dataframes.
Dataframes are imported using Pandas and formatted for process mining usage using the
pm4py.format_dataframe(df: pd.DataFrame, case_id: str = constants.CASE_CONCEPT_NAME, activity_key: str = xes_constants.DEFAULT_NAME_KEY, timestamp_key: str = xes_constants.DEFAULT_TIMESTAMP_KEY, start_timestamp_key: str = xes_constants.DEFAULT_START_TIMESTAMP_KEY, timest_format: Optional[str] = None) -> pd.DataFrame
method. The more standard dataframe structure follows:
 #   Column             Non-Null Count  Dtype
---  ------             --------------  -----
 0   case:concept:name  8577 non-null   string
 1   concept:name       8577 non-null   string
 2   time:timestamp     8577 non-null   datetime64[ns, UTC]
 3   org:resource       8577 non-null   object
In which "case:concept:name" is the case identifier, "concept:name" is the activity, "time:timestamp" is the timestamp, and "org:resource" is the resource.

Previously, we were using a more complex data structure:
- An EventLog object (imported from pm4py.objects.log.obj) contains a dictionary of attributes at the log level (log.attributes) and provides an iterator over its traces (for trace in log:)
- A Trace object (imported from pm4py.objects.log.obj) contains a dictionary of attributes at the trace level (trace.attributes) and provides an iterator over its events (for event in trace:). The most important attribute at the trace level is concept:name (the case ID)
- An Event object is a dictionary associating to some attributes their values. The most important attributes at the event level are concept:name (the activity), time:timestamp (the timestamp), org:resource (the resource).

Also, we were offering an EventStream which is simply a list of events (Event).
Pandas dataframes, event streams and event logs can be converted from/to each other.

pm4py does not offer a method to directly read CSVs. Instead, Pandas (pd.read_csv) must be used.

TRADITIONAL PROCESS MODELS

DIRECTLY-FOLLOWS GRAPH is a dictionary associating to each couple of activities the corresponding frequency, example
{('check ticket', 'decide'): 6, ('check ticket', 'examine casually'): 2, ('check ticket', 'examine thoroughly'): 1}


PERFORMANCE DIRECTLY-FOLLOWS GRAPH is a dictionary associating to each couple of activities different aggregations of the time between them, example
{('check ticket', 'decide'): {'mean': 181960.0, 'median': 129300.0, 'max': 578640.0, 'min': 1800.0, 'sum': 1091760.0, 'stdev': 206034.11756308711}, ('check ticket', 'examine casually'): {'mean': 92430.0, 'median': 92430.0, 'max': 177420.0, 'min': 7440.0, 'sum': 184860.0, 'stdev': 120194.01066608935}, ('check ticket', 'examine thoroughly'): {'mean': 95820.0, 'median': 95820.0, 'max': 95820.0, 'min': 95820.0, 'sum': 95820.0, 'stdev': nan}

PETRI NET
A Petri net is defined in the class pm4py.objects.petri_net.obj.PetriNet. A name (string unique identifier) must be provided to the constructor.
The main properties of this bipartite graph are:
- places (set). A place is defined in the class pm4py.objects.petri_net.obj.PetriNet.Place. A name (string unique identifier) must be provided to the constructor. Places must be added to the net.places set
- transitions (set). A transition is defined in the class pm4py.objects.petri_net.obj.PetriNet.Transition. A name (string unique identifier) and a label (string) must be provided to the constructor. Places must be added to the net.transitions set
- arcs (set; an arc can be between a place and a transition, or a transition and a place). An arc must be added using the method pm4py.objects.petri_net.utils.petri_utils.add_arc_from_to(fr, to, net: PetriNet, weight=1, type=None) -> PetriNet.Arc.
The sets of input/output arcs for the places/transitions are also reported in the in_arcs and out_arcs property of them, not only in the arcs property of the Petri net.

To remove places, transitions and arcs from the Petri nets, you need to carefully use the following methods:
- pm4py.objects.petri_net.utils.petri_utils.remove_place(net: PetriNet, place: PetriNet.Place) -> PetriNet
- pm4py.remove_transition(net: PetriNet, trans: PetriNet.Transition) -> PetriNet
- pm4py.remove_arc(net: PetriNet, arc: PetriNet.Arc) -> PetriNet

ACCEPTING PETRI NET
A Petri net plus an initial and a final marking is an accepting Petri net.
The markings are dictionaries defined in the class pm4py.objects.petri_net.obj.Marking.
They associate to a subset of places of the Petri net the corresponding number of tokens.
The initial marking is the initial state of the Petri net, while the final marking is the final state of the Petri net.
Example construction and distruction of an accepting Petri net:

from pm4py.objects.petri_net.obj import PetriNet, Marking
from pm4py.objects.petri_net.utils import petri_utils
net = PetriNet(name="example")
source = PetriNet.Place(name="source")
sink = PetriNet.Place(name="sink")
execute_activity = PetriNet.Transition(name="execute_activity", label="Execute Activity")
net.places.add(source)
net.places.add(sink)
net.transitions.add(execute_activity)
petri_utils.add_arc_from_to(source, execute_activity, net)
petri_utils.add_arc_from_to(execute_activity, sink, net)
im = Marking()
im[source] = 1
fm = Marking()
fm[sink] = 1
# finally, remove the sink place
petri_utils.remove_place(net, sink)
del fm[sink]


PROCESS TREE
A process tree is a hierarchical process model.
The following operators are defined for process trees:
-> ( A, B ) tells that the process tree A should be executed before the process tree B
X ( A, B ) tells that there is an exclusive choice between executing the process tree A or the process tree B
+ ( A, B ) tells that A and B are executed in true concurrency.
* ( A, B ) is a loop. So the process tree A is executed, then either you exit the loop, or you execute B and then A again (this can happen several times until the loop is exited).
the leafs of a process tree are either activities (denoted by 'X' where X is the name of the activity) or silent steps (indicated by tau).
An example process tree follows:
+ ( 'A', -> ( 'B', 'C' ) )
tells that you should execute B before executing C. In true concurrency, you can execute A. So the possible traces are A->B->C, B->A->C, B->C->A.

ProcessTree objects are defined in pm4py.objects.process_tree.obj.ProcessTree
They have as properties:
- parent (the parent process tree, which is left empty for the root node)
- children (the child ProcessTree objects)
- operator (one of the pm4py.objects.process_tree.obj.Operator enumeration values: Operator.SEQUENCE, Operator.XOR, Operator.PARALLEL, Operator.LOOP)
- label (if the ProcessTree is a leaf, then it is valued with the label)
The properties are mimicked in the constructor.

Note that adding the process trees as children is not enough. Their parent should be explicitly set to the parent node. Otherwise, it does not work.

Example construction:
from pm4py.objects.process_tree.obj import ProcessTree, Operator
root = ProcessTree(operator=Operator.PARALLEL)
A = ProcessTree(label="A", parent=root)
seq = ProcessTree(operator=Operator.SEQUENCE, parent=root)
B = ProcessTree(label="B", parent=seq)
C = ProcessTree(label="C", parent=seq)
seq.children.append(B)
seq.children.append(C)
root.children.append(A)
root.children.append(seq)


POWL (PARTIALLY ORDERED WORKFLOW LANGUAGE) MODELS
A partially ordered workflow language (POWL) is a partially ordered graph representation of a process, extended with control-flow operators for modeling choice and loop structures. There are four types of POWL models:
- an activity (identified by its label, i.e., 'M' identifies the activity M). Silent activities with empty labels (tau labels) are also supported.
- a choice of other POWL models (an exclusive choice between the sub-models A and B is identified by X ( A, B ) )
- a loop node between two POWL models (a loop between the sub-models A and B is identified by * ( A, B ) and tells that you execute A, then you either exit the loop or execute B and then A again, this is repeated until you exit the loop).
- a partial order over a set of POWL models. A partial order is a binary relation that is irreflexive, transitive, and asymmetric. A partial order sets an execution order between the sub-models (i.e., the target node cannot be executed before the source node is completed). Unconnected nodes in a partial order are considered to be concurrent. An example is PO=(nodes={ NODE1, NODE2 }, order={ })
where NODE1 and NODE2 are independent and can be executed in parallel. Another example is PO=(nodes={ NODE1, NODE2 }, order={ NODE1-->NODE2 }) where NODE2 can only be executed after NODE1 is completed.
A more advanced example: PO=(nodes={ NODE1, NODE2, NODE3, X ( NODE4, NODE5 ) }, order={ NODE1-->NODE2, NODE1-->X ( NODE4, NODE5 ), NODE2-->X ( NODE4, NODE5 ) }), in this case, NODE2 can be executed only after NODE1 is completed, while the choice between NODE4 and NODE5 needs to wait until both NODE1 and NODE2 are finalized.

POWL models are defined in pm4py.objects.powl.obj in different classes:
- SilentTransition defines a silent transition (without operator and without label)
- Transition defines a transition with label (without operator)
- StrictPartialOrder defines a POWL model with a main property: nodes (the children POWL models). The order between elements can be added with the method .order.add_edge(source_node, target_node). The nodes must be provided in the constructor, and cannot be modified afterwards.
- OperatorPOWL defines a POWL model with two main properties: children (the children POWL models) and operator (which can be either pm4py.objects.process_tree.obj.Operator.XOR or pm4py.objects.process_tree.obj.Operator.LOOP). The children must be provided in the constructor, and cannot be modified afterwards.
The properties are mimcked in the construtors. In this example, a POWL model is constructed where a loop between A and B is followed by either C or a silent transition.

import pm4py
from pm4py.objects.powl.obj import StrictPartialOrder, OperatorPOWL, Transition, SilentTransition
from pm4py.objects.process_tree.obj import Operator
A = Transition(label="A")
B = Transition(label="B")
C = Transition(label="C")
skip = SilentTransition()
loop = OperatorPOWL(operator=Operator.LOOP, children=[A, B])
xor = OperatorPOWL(operator=Operator.XOR, children=[C, skip])
root = StrictPartialOrder(nodes=[loop, xor])
root.order.add_edge(loop, xor)


LOG SKELETON
The Log Skeleton process model contains the following declarative constraints:
- Equivalence (if the first activity occurs, then it has the same occurrences as the second one)
- Always Before (if the first activity occur, then the second activity should have been executed previously)
- Always After (if the first activity occur, then the second activity is executed in one of the following events)
- Never Together (the two activities cannot co-exist inside the same case)
- Activity Occurrences (bounds the number of occurrences for an activity in a case)
- Directly-Follows Constraints (if the first activity occurs, then the second activity shall occur immediately after)
The Log Skeleton is expressed as a Python dictionary containing the keys: 'equivalence', 'always_before', 'always_after', 'never_together', 'activ_freq', 'directly_follows'.
The values associated to 'equivalence', 'always_before', 'always_after', 'never_together', 'directly_follows' are sets containing the couples of activities satisfying the constraints.
The value associated to 'activ_freq' is a dictionary whose keys are the activities, and the values are the allowed number of occurrences for the given activity. For example, {'A': {0, 1}, 'B': {1, 2}} tells that A could occur 0 or 1 time inside a case, while B could occur 1 or 2 times.


DECLARE
A DECLARE model in pm4py is expressed as a Python dictionary containing the following keys:
'existence', 'absence', 'exactly_one', 'init', 'responded_existence', 'coexistence', 'response', 'precedence', 'succession', 'altresponse', 'altprecedence', 'altsuccession', 'chainresponse', 'chainprecedence', 'chainsuccession', 'noncoexistence', 'nonsuccession', 'nonchainsuccession'
For the keys {'existence', 'absence', 'exactly_one', 'init'}, the value is a dictionary containing as keys the activities and as corresponding value the support (please set it to 1.0) and confidence of the declarative rule.
For the keys {'responded_existence', 'coexistence', 'response', 'precedence', 'succession', 'altresponse', 'altprecedence', 'altsuccession', 'chainresponse', 'chainprecedence', 'chainsuccession', 'noncoexistence', 'nonsuccession', 'nonchainsuccession'}, the value is a dictionary containing as keys the activities and as corresponding value the support (please set it to 1.0) and confidence of the declarative rule.


TEMPORAL PROFILE
The temporal profile is a model describing the average and the standard deviation of the times between couples of activities eventually (not only directly) following each other in at least a process execution (so in a trace <A,B,C,D> the couples (A,B) (A,C) (A,D) (B,C) (B,D) (C,D) shall be considered). Given a positive value ZETA, a deviation occurs in a process execution when the time between two activities is lower than AVG - ZETA * STDEV or greater than AVG + ZETA * STDEV.
The temporal profile is expressed as a Python dictionary associating to some couples of activities the average and the standard deviation of the times. Example: {('A', 'B'): (86400, 3600), ('B', 'C'): (3600, 3600)} indicates that the average time between A and B is 1 day, while the standard deviation is 1 hour. On the other hand, the average time between B and C is 1 hour, while the standard deviation is 1 hour.


Aside from object-centric event logs, the following methods are available in pm4py:

pm4py.format_dataframe(df: pd.DataFrame, case_id: str = constants.CASE_CONCEPT_NAME, activity_key: str = xes_constants.DEFAULT_NAME_KEY, timestamp_key: str = xes_constants.DEFAULT_TIMESTAMP_KEY, start_timestamp_key: str = xes_constants.DEFAULT_START_TIMESTAMP_KEY, timest_format: Optional[str] = None) -> pd.DataFrame
Give the appropriate format on the dataframe, for process mining purposes.

pm4py.parse_process_tree(tree_string: str) -> ProcessTree
Parse a process tree from a string
:param tree_string: String representing a process tree (e.g. '-> ( 'A', O ( 'B', 'C' ), 'D' )'). Operators are '->': sequence, '+': parallel, 'X': xor choice, '*': binary loop, 'O' or choice

pm4py.parse_powl_model_string(powl_string: str) -> POWL
Parse a POWL model from a string representation of the process model

pm4py.parse_event_log_string(traces: Collection[str], sep: str = ",", activity_key: str = xes_constants.DEFAULT_NAME_KEY, timestamp_key: str = xes_constants.DEFAULT_TIMESTAMP_KEY, case_id_key: str = constants.CASE_CONCEPT_NAME, return_legacy_log_object: bool = constants.DEFAULT_READ_XES_LEGACY_OBJECT) -> Union[EventLog, pd.DataFrame]
Parse a collection of traces expressed as strings (e.g., ["A,B,C,D", "A,C,B,D", "A,D"]) to a log object (Pandas dataframe)

pm4py.project_on_event_attribute(log: Union[EventLog, pd.DataFrame], attribute_key=xes_constants.DEFAULT_NAME_KEY, case_id_key=None) -> List[List[str]]
Project the event log on a specified event attribute. The result is a list, containing a list for each case: all the cases are transformed to list of values for the specified attribute.

pm4py.sample_cases(log: Union[EventLog, pd.DataFrame], num_cases: int, case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
(Random) Sample a given number of cases from the event log.

pm4py.sample_events(log: Union[EventStream, OCEL], num_events: int) -> Union[EventStream, OCEL, pd.DataFrame]
(Random) Sample a given number of events from the event log.

pm4py.discover_dfg(log: Union[EventLog, pd.DataFrame], activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Tuple[dict, dict, dict]
Discovers a Directly-Follows Graph (DFG) from a log. This method returns a dictionary with the couples of directly-following activities (in the log) as keys and the frequency of relation as value.

pm4py.discover_performance_dfg(log: Union[EventLog, pd.DataFrame], business_hours: bool = False, business_hour_slots=constants.DEFAULT_BUSINESS_HOUR_SLOTS, workcalendar=constants.DEFAULT_BUSINESS_HOURS_WORKCALENDAR, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Tuple[dict, dict, dict]
Discovers a performance directly-follows graph from an event log. This method returns a dictionary with the couples of directly-following activities (in the log) as keys and the performance of relation as value.

pm4py.discover_petri_net_alpha(log: Union[EventLog, pd.DataFrame], activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Tuple[PetriNet, Marking, Marking]
Discovers a Petri net using the Alpha Miner.

pm4py.discover_petri_net_ilp(log: Union[EventLog, pd.DataFrame], alpha: float = 1.0, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Tuple[PetriNet, Marking, Marking]
Discovers a Petri net using the ILP Miner.

pm4py.discover_petri_net_alpha_plus(log: Union[EventLog, pd.DataFrame], activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Tuple[PetriNet, Marking, Marking]
Discovers a Petri net using the Alpha+ algorithm.

pm4py.discover_petri_net_inductive(log: Union[EventLog, pd.DataFrame, DFG], multi_processing: bool = constants.ENABLE_MULTIPROCESSING_DEFAULT, noise_threshold: float = 0.0, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", disable_fallthroughs: bool = False) -> Tuple[PetriNet, Marking, Marking]
Discovers a Petri net using the inductive miner algorithm.

pm4py.discover_petri_net_heuristics(log: Union[EventLog, pd.DataFrame], dependency_threshold: float = 0.5, and_threshold: float = 0.65, loop_two_threshold: float = 0.5, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Tuple[PetriNet, Marking, Marking]
Discover a Petri net using the Heuristics Miner.

pm4py.discover_process_tree_inductive(log: Union[EventLog, pd.DataFrame, DFG], noise_threshold: float = 0.0, multi_processing: bool = constants.ENABLE_MULTIPROCESSING_DEFAULT, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", disable_fallthroughs: bool = False) -> ProcessTree
Discovers a process tree using the inductive miner algorithm.

pm4py.discover_heuristics_net(log: Union[EventLog, pd.DataFrame], dependency_threshold: float = 0.5, and_threshold: float = 0.65, loop_two_threshold: float = 0.5, min_act_count: int = 1, min_dfg_occurrences: int = 1, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", decoration: str = "frequency") -> HeuristicsNet
Discovers an heuristics net.

pm4py.derive_minimum_self_distance(log: Union[DataFrame, EventLog, EventStream], activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Dict[str, int]
This algorithm computes the minimum self-distance for each activity observed in an event log. The self distance of a in <a> is infinity, of a in <a,a> is 0, in <a,b,a> is 1, etc. The activity key 'concept:name' is used.

pm4py.discover_footprints(*args: Union[EventLog, Tuple[PetriNet, Marking, Marking], ProcessTree]) -> Union[List[Dict[str, Any]], Dict[str, Any]]
Discovers the footprints out of the provided event log / process model.

pm4py.discover_eventually_follows_graph(log: Union[EventLog, pd.DataFrame], activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Dict[Tuple[str, str], int]
Gets the eventually follows graph from a log object.

pm4py.discover_bpmn_inductive(log: Union[EventLog, pd.DataFrame, DFG], noise_threshold: float = 0.0, multi_processing: bool = constants.ENABLE_MULTIPROCESSING_DEFAULT, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", disable_fallthroughs: bool = False) -> BPMN
Discovers a BPMN using the Inductive Miner algorithm.

pm4py.discover_transition_system(log: Union[EventLog, pd.DataFrame], direction: str = "forward", window: int = 2, view: str = "sequence", activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> TransitionSystem
Discovers a transition system as described in the process mining book "Process Mining: Data Science in Action"

pm4py.discover_prefix_tree(log: Union[EventLog, pd.DataFrame], activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Trie
Discovers a prefix tree from the provided log object.

pm4py.discover_temporal_profile(log: Union[EventLog, pd.DataFrame], activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Dict[Tuple[str, str], Tuple[float, float]]
Discovers a temporal profile from a log object. The output is a dictionary containing, for every couple of activities eventually following in at least a case of the log, the average and the standard deviation of the difference of the timestamps.

pm4py.discover_log_skeleton(log: Union[EventLog, pd.DataFrame], noise_threshold: float = 0.0, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Dict[str, Any]
Discovers a log skeleton from an event log.

pm4py.discover_declare(log: Union[EventLog, pd.DataFrame], allowed_templates: Optional[Set[str]] = None, considered_activities: Optional[Set[str]] = None, min_support_ratio: Optional[float] = None, min_confidence_ratio: Optional[float] = None, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Dict[str, Dict[Any, Dict[str, int]]]
Discovers a DECLARE model from an event log.

pm4py.discover_powl(log: Union[EventLog, pd.DataFrame], variant=POWLDiscoveryVariant.MAXIMAL, filtering_weight_factor: float = 0.0, order_graph_filtering_threshold: float = None, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> POWL
Discovers a POWL model from an event log.

pm4py.discover_batches(log: Union[EventLog, pd.DataFrame], merge_distance: int = 15 * 60, min_batch_size: int = 2, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", resource_key: str = "org:resource") -> List[Tuple[Tuple[str, str], int, Dict[str, Any]]]
Discover batches from the provided log object. We say that an activity is executed in batches by a given resource when the resource executes several times the same activity in a short period of time. Identifying such activities may identify points of the process that can be automated, since the activity of the person may be repetitive. The following categories of batches are detected: Simultaneous (all the events in the batch have identical start and end timestamps); Batching at start (all the events in the batch have identical start timestamp); Batching at end (all the events in the batch have identical end timestamp); Sequential batching (for all the consecutive events, the end of the first is equal to the start of the second); Concurrent batching (for all the consecutive events that are not sequentially matched);

pm4py.conformance_diagnostics_token_based_replay(log: Union[EventLog, pd.DataFrame], petri_net: PetriNet, initial_marking: Marking, final_marking: Marking, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", return_diagnostics_dataframe: bool = constants.DEFAULT_RETURN_DIAGNOSTICS_DATAFRAME, opt_parameters: Optional[Dict[Any, Any]] = None) -> List[Dict[str, Any]]
Apply token-based replay for conformance checking analysis. The methods return the full token-based-replay diagnostics. Token-based replay matches a trace and a Petri net model, starting from the initial place, in order to discover which transitions are executed and in which places we have remaining or missing tokens for the given process instance. Token-based replay is useful for Conformance Checking: indeed, a trace is fitting according to the model if, during its execution, the transitions can be fired without the need to insert any missing token. If the reaching of the final marking is imposed, then a trace is fitting if it reaches the final marking without any missing or remaining tokens. The output of the token-based replay, stored in the variable replayed_traces, contains for each trace of the log: trace_is_fit: boolean value (True/False) that is true when the trace is according to the model; activated_transitions: list of transitions activated in the model by the token-based replay; reached_marking: marking reached at the end of the replay; missing_tokens: number of missing tokens; consumed_tokens: number of consumed tokens; remaining_tokens: number of remaining tokens; produced_tokens: number of produced tokens;

pm4py.conformance_diagnostics_alignments(log: Union[EventLog, pd.DataFrame], *args, multi_processing: bool = constants.ENABLE_MULTIPROCESSING_DEFAULT, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", variant_str : Optional[str] = None, return_diagnostics_dataframe: bool = constants.DEFAULT_RETURN_DIAGNOSTICS_DATAFRAME, **kwargs) -> List[Dict[str, Any]]
Apply the alignments algorithm between a log and a process model. The methods return the full alignment diagnostics. Alignment-based replay aims to find one of the best alignment between the trace and the model. For each trace, the output of an alignment is a list of couples where the first element is an event (of the trace) or » and the second element is a transition (of the model) or ». For each couple, the following classification could be provided: Sync move: the classification of the event corresponds to the transition label; in this case, both the trace and the model advance in the same way during the replay. Move on log: for couples where the second element is », it corresponds to a replay move in the trace that is not mimicked in the model. This kind of move is unfit and signal a deviation between the trace and the model. Move on model: for couples where the first element is », it corresponds to a replay move in the model that is not mimicked in the trace. For moves on model, we can have the following distinction: Moves on model involving hidden transitions: in this case, even if it is not a sync move, the move is fit. Moves on model not involving hidden transitions: in this case, the move is unfit and signals a deviation between the trace and the model. With each trace, a dictionary containing among the others the following information is associated:

pm4py.fitness_token_based_replay(log: Union[EventLog, pd.DataFrame], petri_net: PetriNet, initial_marking: Marking, final_marking: Marking, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Dict[str, float]
Calculates the fitness using token-based replay. The fitness is calculated on a log-based level.
The output dictionary contains the following keys:
- average_trace_fitness (between 0.0 and 1.0; computed as average of the trace fitnesses)
- log_fitness (between 0.0 and 1.0)
- percentage_of_fitting_traces (the percentage of fit traces (from 0.0 to 100.0)

pm4py.fitness_alignments(log: Union[EventLog, pd.DataFrame], petri_net: PetriNet, initial_marking: Marking, final_marking: Marking, multi_processing: bool = constants.ENABLE_MULTIPROCESSING_DEFAULT, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", variant_str : Optional[str] = None) -> Dict[str, float]
Calculates the fitness using alignments.
The output dictionary contains the following keys:
- average_trace_fitness (between 0.0 and 1.0; computed as average of the trace fitnesses)
- log_fitness (between 0.0 and 1.0)
- percentage_of_fitting_traces (the percentage of fit traces (from 0.0 to 100.0)

pm4py.precision_token_based_replay(log: Union[EventLog, pd.DataFrame], petri_net: PetriNet, initial_marking: Marking, final_marking: Marking, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> float
Calculates the precision precision using token-based replay.

pm4py.precision_alignments(log: Union[EventLog, pd.DataFrame], petri_net: PetriNet, initial_marking: Marking, final_marking: Marking, multi_processing: bool = constants.ENABLE_MULTIPROCESSING_DEFAULT, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> float
Calculates the precision of the model w.r.t. the event log using alignments.

pm4py.generalization_tbr(log: Union[EventLog, pd.DataFrame], petri_net: PetriNet, initial_marking: Marking, final_marking: Marking, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> float
Computes the generalization of the model (against the event log).

pm4py.conformance_temporal_profile(log: Union[EventLog, pd.DataFrame], temporal_profile: Dict[Tuple[str, str], Tuple[float, float]], zeta: float = 1.0, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", return_diagnostics_dataframe: bool = constants.DEFAULT_RETURN_DIAGNOSTICS_DATAFRAME) -> List[List[Tuple[float, float, float, float]]]
Performs conformance checking on the provided log with the provided temporal profile. The result is a list of time-based deviations for every case. E.g. if the log on top of which the conformance is applied is the following (1 case): A (timestamp: 2000-01)    B (timestamp: 2002-01) The difference between the timestamps of A and B is two years. If the temporal profile: {('A', 'B'): (1.5 months, 0.5 months), ('A', 'C'): (5 months, 0), ('A', 'D'): (2 months, 0)} is specified, and zeta is set to 1, then the aforementioned case would be deviating (considering the couple of activities ('A', 'B')), because 2 years > 1.5 months + 0.5 months.

pm4py.conformance_declare(log: Union[EventLog, pd.DataFrame], declare_model: Dict[str, Dict[Any, Dict[str, int]]], activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", return_diagnostics_dataframe: bool = constants.DEFAULT_RETURN_DIAGNOSTICS_DATAFRAME) -> List[Dict[str, Any]]
Applies conformance checking against a DECLARE model.

pm4py.conformance_log_skeleton(log: Union[EventLog, pd.DataFrame], log_skeleton: Dict[str, Any], activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", return_diagnostics_dataframe: bool = constants.DEFAULT_RETURN_DIAGNOSTICS_DATAFRAME) -> List[Set[Any]]
Performs conformance checking using the log skeleton.

pm4py.filter_log_relative_occurrence_event_attribute(log: Union[EventLog, pd.DataFrame], min_relative_stake: float, attribute_key : str = xes_constants.DEFAULT_NAME_KEY, level="cases", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Filters the event log keeping only the events having an attribute value which occurs: - in at least the specified (min_relative_stake) percentage of events, when level="events" - in at least the specified (min_relative_stake) percentage of cases, when level="cases"

pm4py.filter_start_activities(log: Union[EventLog, pd.DataFrame], activities: Union[Set[str], List[str]], retain: bool = True, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Filter cases having a start activity in the provided list

pm4py.filter_end_activities(log: Union[EventLog, pd.DataFrame], activities:  Union[Set[str], List[str]], retain: bool = True, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Filter cases having an end activity in the provided list

pm4py.filter_event_attribute_values(log: Union[EventLog, pd.DataFrame], attribute_key: str, values:  Union[Set[str], List[str]], level: str = "case", retain: bool = True, case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Filter a log object on the values of some event attribute

pm4py.filter_trace_attribute_values(log: Union[EventLog, pd.DataFrame], attribute_key: str, values:  Union[Set[str], List[str]], retain: bool = True, case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Filter a log on the values of a trace attribute

pm4py.filter_variants(log: Union[EventLog, pd.DataFrame], variants:  Union[Set[str], List[str], List[Tuple[str]]], retain: bool = True, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Filter a log on a specified set of variants

pm4py.filter_directly_follows_relation(log: Union[EventLog, pd.DataFrame], relations: List[str], retain: bool = True, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Retain traces that contain any of the specified 'directly follows' relations. For example, if relations == [('a','b'),('a','c')] and log [<a,b,c>,<a,c,b>,<a,d,b>] the resulting log will contain traces describing [<a,b,c>,<a,c,b>].

pm4py.filter_eventually_follows_relation(log: Union[EventLog, pd.DataFrame], relations: List[str], retain: bool = True, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Retain traces that contain any of the specified 'eventually follows' relations. For example, if relations == [('a','b'),('a','c')] and log [<a,b,c>,<a,c,b>,<a,d,b>] the resulting log will contain traces describing [<a,b,c>,<a,c,b>,<a,d,b>].

pm4py.filter_time_range(log: Union[EventLog, pd.DataFrame], dt1: str, dt2: str, mode="events", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Filter a log on a time interval
:param mode: modality of filtering (events, traces_contained, traces_intersecting). events: any event that fits the time frame is retained; traces_contained: any trace completely contained in the timeframe is retained; traces_intersecting: any trace intersecting with the time-frame is retained.

pm4py.filter_between(log: Union[EventLog, pd.DataFrame], act1: Union[str, List[str]], act2: Union[str, List[str]], activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Finds all the sub-cases leading from an event with activity "act1" to an event with activity "act2" in the log, and returns a log containing only them.

pm4py.filter_case_size(log: Union[EventLog, pd.DataFrame], min_size: int, max_size: int, case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Filters the event log, keeping the cases having a length (number of events) included between min_size and max_size

pm4py.filter_case_performance(log: Union[EventLog, pd.DataFrame], min_performance: float, max_performance: float, timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Filters the event log, keeping the cases having a duration (the timestamp of the last event minus the timestamp of the first event) included between min_performance and max_performance

pm4py.filter_activities_rework(log: Union[EventLog, pd.DataFrame], activity: str, min_occurrences: int = 2, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Filters the event log, keeping the cases where the specified activity occurs at least min_occurrences times.

pm4py.filter_paths_performance(log: Union[EventLog, pd.DataFrame], path: Tuple[str, str], min_performance: float, max_performance: float, keep=True, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Filters the event log, either: (keep=True) keeping the cases having the specified path (tuple of 2 activities) with a duration included between min_performance and max_performance; (keep=False) discarding the cases having the specified path with a duration included between min_performance and max_performance

pm4py.filter_variants_top_k(log: Union[EventLog, pd.DataFrame], k: int, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Keeps the top-k variants of the log

pm4py.filter_variants_by_coverage_percentage(log: Union[EventLog, pd.DataFrame], min_coverage_percentage: float, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Filters the variants of the log by a coverage percentage (e.g., if min_coverage_percentage=0.4, and we have a log with 1000 cases, of which 500 of the variant 1, 400 of the variant 2, and 100 of the variant 3, the filter keeps only the traces of variant 1 and variant 2).

pm4py.filter_prefixes(log: Union[EventLog, pd.DataFrame], activity: str, strict=True, first_or_last="first", activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Filters the log, keeping the prefixes to a given activity.

pm4py.filter_suffixes(log: Union[EventLog, pd.DataFrame], activity: str, strict=True, first_or_last="first", activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Filters the log, keeping the suffixes from a given activity.

pm4py.filter_four_eyes_principle(log: Union[EventLog, pd.DataFrame], activity1: str, activity2: str, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", resource_key: str = "org:resource") -> Union[EventLog, pd.DataFrame]
Filter the cases of the log which violates the four eyes principle on the provided activities.

pm4py.filter_activity_done_different_resources(log: Union[EventLog, pd.DataFrame], activity: str, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", resource_key: str = "org:resource") -> Union[EventLog, pd.DataFrame]
Filters the cases where an activity is repeated by different resources.

pm4py.filter_trace_segments(log: Union[EventLog, pd.DataFrame], admitted_traces: List[List[str]], positive: bool = True, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Filters an event log on a set of traces. A trace is a sequence of activities and "...", in which:
- a "..." before an activity tells that other activities can precede the given activity
- a "..." after an activity tells that other activities can follow the given activity
For example:
- pm4py.filter_trace_segments(log, [["A", "B"]]) <- filters only the cases of the event log having exactly the process variant A,B
- pm4py.filter_trace_segments(log, [["...", "A", "B"]]) <- filters only the cases of the event log ending with the activities A,B
- pm4py.filter_trace_segments(log, [["A", "B", "..."]]) <- filters only the cases of the event log starting with the activities A,B
- pm4py.filter_trace_segments(log, [["...", "A", "B", "C", "..."], ["...", "D", "E", "F", "..."]] <- filters only the cases of the event log in which at any point there is A followed by B followed by C, and in which at any other point there is D followed by E followed by F

pm4py.read_xes(file_path: str, variant: Optional[str] = None, return_legacy_log_object: bool = constants.DEFAULT_READ_XES_LEGACY_OBJECT, encoding: str = constants.DEFAULT_ENCODING, **kwargs) -> Union[DataFrame, EventLog]
Reads an event log stored in XES format (see `xes-standard <https://xes-standard.org/>`_) Returns a table (``pandas.DataFrame``) view of the event log.

pm4py.read_pnml(file_path: str, auto_guess_final_marking: bool = False, encoding: str = constants.DEFAULT_ENCODING) -> Tuple[PetriNet, Marking, Marking]
Reads a Petri net object from a .pnml file.

pm4py.read_ptml(file_path: str, encoding: str = constants.DEFAULT_ENCODING) -> ProcessTree
Reads a process tree object from a .ptml file

pm4py.read_dfg(file_path: str, encoding: str = constants.DEFAULT_ENCODING) -> Tuple[Dict[Tuple[str,str],int], Dict[str,int], Dict[str,int]]
Reads a DFG object from a .dfg file.

pm4py.read_bpmn(file_path: str, encoding: str = constants.DEFAULT_ENCODING) -> BPMN
Reads a BPMN model from a .bpmn file

pm4py.write_xes(log: Union[EventLog, pd.DataFrame], file_path: str, case_id_key: str = "case:concept:name", extensions: Optional[Collection[XESExtension]] = None, encoding: str = constants.DEFAULT_ENCODING, **kwargs)
Writes an event log to disk in the XES format (see `xes-standard <https://xes-standard.org/>`_)

pm4py.write_pnml(petri_net: PetriNet, initial_marking: Marking, final_marking: Marking, file_path: str, encoding: str = constants.DEFAULT_ENCODING)
Writes a Petri net object to disk in the ``.pnml`` format

pm4py.write_ptml(tree: ProcessTree, file_path: str, encoding: str = constants.DEFAULT_ENCODING)
Writes a process tree object to disk in the ``.ptml`` format

pm4py.write_dfg(dfg: Dict[Tuple[str,str],int], start_activities:  Dict[str,int], end_activities:  Dict[str,int], file_path: str, encoding: str = constants.DEFAULT_ENCODING)
Writes a directly follows graph (DFG) object to disk in the ``.dfg`` format

pm4py.write_bpmn(model: BPMN, file_path: str, auto_layout: bool = True, encoding: str = constants.DEFAULT_ENCODING)
Writes a BPMN model object to disk in the ``.bpmn`` format

pm4py.convert_to_event_log(obj: Union[pd.DataFrame, EventStream], case_id_key: str = "case:concept:name", **kwargs) -> EventLog
Converts a DataFrame/EventStream object to an event log object

pm4py.convert_to_event_stream(obj: Union[EventLog, pd.DataFrame], case_id_key: str = "case:concept:name", **kwargs) -> EventStream
Converts a log object to an event stream

pm4py.convert_to_dataframe(obj: Union[EventStream, EventLog], **kwargs) -> pd.DataFrame
Converts a log object to a dataframe

pm4py.convert_to_bpmn(*args: Union[Tuple[PetriNet, Marking, Marking], ProcessTree]) -> BPMN
Converts an object to a BPMN diagram. As an input, either a Petri net (with corresponding initial and final marking) or a process tree can be provided. A process tree can always be converted into a BPMN model and thus quality of the result object is guaranteed. For Petri nets, the quality of the converison largely depends on the net provided (e.g., sound WF-nets are likely to produce reasonable BPMN models)

pm4py.convert_to_petri_net(*args: Union[BPMN, ProcessTree, HeuristicsNet, POWL, dict]) -> Tuple[PetriNet, Marking, Marking]
Converts an input model to an (accepting) Petri net. The input objects can either be a process tree, BPMN model or a Heuristic net. The output is a triple, containing the Petri net and the initial and final markings. The markings are only returned if they can be reasonable derived from the input model.

pm4py.convert_to_process_tree(*args: Union[Tuple[PetriNet, Marking, Marking], BPMN]) -> ProcessTree
Converts an input model to a process tree. The input models can either be Petri nets (marked) or BPMN models. For both input types, the conversion is not guaranteed to work, hence, invocation of the method can yield an Exception.

pm4py.convert_to_reachability_graph(*args: Union[Tuple[PetriNet, Marking, Marking], BPMN, ProcessTree]) -> TransitionSystem
Converts an input model to a reachability graph (transition system). The input models can either be Petri nets (with markings), BPMN models or process trees. The output is the state-space of the model (i.e., the reachability graph), enocdoed as a ``TransitionSystem`` object.

pm4py.convert_log_to_networkx(log: Union[EventLog, EventStream, pd.DataFrame], include_df: bool = True, case_id_key: str = "concept:name", other_case_attributes_as_nodes: Optional[Collection[str]] = None, event_attributes_as_nodes: Optional[Collection[str]] = None) -> nx.DiGraph
Converts an event log object to a NetworkX DiGraph object. The nodes of the graph are the events, the cases (and possibly the attributes of the log). The edges are: Connecting each event to the corresponding case (BELONGS_TO type); Connecting every event to the directly-following one (DF type, if enabled); Connecting every case/event to the given attribute values (ATTRIBUTE_EDGE type)

pm4py.convert_log_to_time_intervals(log: Union[EventLog, pd.DataFrame], filter_activity_couple: Optional[Tuple[str, str]] = None, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", start_timestamp_key: str = "time:timestamp") -> List[List[Any]]
Gets a list of intervals from an event log. Each interval contains two temporally consecutive events and measures the time between the two events (complete timestamp of the first against start timestamp of the second).

pm4py.convert_petri_net_to_networkx(net: PetriNet, im: Marking, fm: Marking) -> nx.DiGraph
Converts a Petri net to a NetworkX DiGraph. Each place and transition is corresponding to a node in the graph.

pm4py.convert_petri_net_type(net: PetriNet, im: Marking, fm: Marking, type: str = "classic") -> Tuple[PetriNet, Marking, Marking]
Changes the Petri net (internal) type

pm4py.play_out(*args: Union[Tuple[PetriNet, Marking, Marking], dict, Counter, ProcessTree], **kwargs) -> EventLog
Performs the playout of the provided model, i.e., gets a set of traces from the model. The function either takes a petri net, initial and final marking, or, a process tree as an input.

pm4py.generate_process_tree(**kwargs) -> ProcessTree
Generates a process tree
Reference paper:
PTandLogGenerator: A Generator for Artificial Event Data
:param kwargs: dictionary containing the parameters of the process tree generator algorithm:
- "mode": most frequent number of visible activities (default 20)
- "min": minimum number of visible activities (default 10)
- "max": maximum number of visible activities (default 30)
- "sequence": probability to add a sequence operator to tree (default 0.25)
- "choice": probability to add a choice operator to tree (default 0.25)
- "parallel": probability to add a parallel operator to tree (default 0.25)
- "loop": probability to add a loop operator to tree (default 0.25)
- "or": probability to add an or operator to tree (default 0.0)
- "silent": probability to add silent activity to a choice or loop operator (default 0.2)
- "duplicate": probability to duplicate an activity label (default 0.0)

pm4py.split_train_test(log: Union[EventLog, pd.DataFrame], train_percentage: float = 0.8, case_id_key="case:concept:name") -> Union[Tuple[EventLog, EventLog], Tuple[pd.DataFrame, pd.DataFrame]]
Split an event log in a training log and a test log (for machine learning purposes). Returns the training and the test event log.

pm4py.get_prefixes_from_log(log: Union[EventLog, pd.DataFrame], length: int, case_id_key: str = "case:concept:name") -> Union[EventLog, pd.DataFrame]
Gets the prefixes of a log of a given length. The returned log object contain the prefixes: if a trace has lower or identical length, it is included as-is; if a trace has greater length, it is cut

pm4py.extract_temporal_features_dataframe(log: Union[EventLog, pd.DataFrame], grouper_freq="W", activity_key="concept:name", timestamp_key="time:timestamp", case_id_key=None, start_timestamp_key="time:timestamp", resource_key="org:resource") -> pd.DataFrame
Extracts a dataframe containing the temporal features of the provided log object. Implements the approach described in the paper: Pourbafrani, Mahsa, Sebastiaan J. van Zelst, and Wil MP van der Aalst. "Supporting automatic system dynamics model generation for simulation in the context of process mining." International Conference on Business Information Systems. Springer, Cham, 2020.

pm4py.discover_handover_of_work_network(log: Union[EventLog, pd.DataFrame], beta=0, resource_key: str = "org:resource", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> SNA
Calculates the handover of work network of the event log. The handover of work network is essentially the DFG of the event log, however, using the resource as a node of the graph, instead of the activity. As such, to use this, resource information should be present in the event log.

pm4py.discover_working_together_network(log: Union[EventLog, pd.DataFrame], resource_key: str = "org:resource", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> SNA
Calculates the working together network of the process. Two nodes resources are connected in the graph if the resources collaborate on an instance of the process.

pm4py.discover_activity_based_resource_similarity(log: Union[EventLog, pd.DataFrame], activity_key: str = "concept:name", resource_key: str = "org:resource", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> SNA
Calculates similarity between the resources in the event log, based on their activity profiles.

pm4py.discover_subcontracting_network(log: Union[EventLog, pd.DataFrame], n=2, resource_key: str = "org:resource", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> SNA
Calculates the subcontracting network of the process.

pm4py.discover_organizational_roles(log: Union[EventLog, pd.DataFrame], activity_key: str = "concept:name", resource_key: str = "org:resource", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> List[Role]
Mines the organizational roles. A role is a set of activities in the log that are executed by a similar (multi)set of resources. Hence, it is a specific function into organization. Grouping the activities in roles can help: Reference paper: Burattin, Andrea, Alessandro Sperduti, and Marco Veluscek. “Business models enhancement through discovery of roles.” 2013 IEEE Symposium on Computational Intelligence and Data Mining (CIDM). IEEE, 2013.

pm4py.discover_network_analysis(log: Union[pd.DataFrame, EventLog, EventStream], out_column: str, in_column: str, node_column_source: str, node_column_target: str, edge_column: str, edge_reference: str = "_out", performance: bool = False, sorting_column: str = xes_constants.DEFAULT_TIMESTAMP_KEY, timestamp_column: str = xes_constants.DEFAULT_TIMESTAMP_KEY) -> Dict[Tuple[str, str], Dict[str, Any]]
Performs a network analysis of the log based on the provided parameters. The classical social network analysis methods are based on the order of the events inside a case. For example, the Handover of Work metric considers the directly-follows relationships between resources during the work of a case. An edge is added between the two resources if such relationships occurs. Real-life scenarios may be more complicated. At first, is difficult to collect events inside the same case without having convergence/divergence issues (see first section of the OCEL part). At second, the type of relationship may also be important. Consider for example the relationship between two resources: this may be more efficient if the activity that is executed is liked by the resources, rather than disgusted. The network analysis that we introduce here generalizes some existing social network analysis metrics, becoming independent from the choice of a case notion and permitting to build a multi-graph instead of a simple graph. With this, we assume events to be linked by signals. An event emits a signal (that is contained as one attribute of the event) that is assumed to be received by other events (also, this is an attribute of these events) that follow the first event in the log. So, we assume there is an OUT attribute (of the event) that is identical to the IN attribute (of the other events). When we collect this information, we can build the network analysis graph: The source node of the relation is given by an aggregation over a node_column_source attribute; The target node of the relation is given by an aggregation over a node_column_target attribute; The type of edge is given by an aggregation over an edge_column attribute; The network analysis graph can either be annotated with frequency or performance information; The output is a multigraph. Two events EV1 and EV2 of the log are merged (indipendently from the case notion) based on having EV1.OUT_COLUMN = EV2.IN_COLUMN. Then, an aggregation is applied on the couple of events (NODE_COLUMN) to obtain the nodes that are connected. The edges between these nodes are aggregated based on some property of the *source* event (EDGE_COLUMN).

pm4py.view_petri_net(petri_net: PetriNet, initial_marking: Optional[Marking] = None, final_marking: Optional[Marking] = None, format: str = constants.DEFAULT_FORMAT_GVIZ_VIEW, bgcolor: str = "white", decorations: Dict[Any, Any] = None, debug: bool = False, rankdir: str = constants.DEFAULT_RANKDIR_GVIZ)
pm4py.save_vis_petri_net(petri_net: PetriNet, initial_marking: Marking, final_marking: Marking, file_path: str, bgcolor: str = "white", decorations: Dict[Any, Any] = None, debug: bool = False, rankdir: str = constants.DEFAULT_RANKDIR_GVIZ, **kwargs)
Views or saves an accepting Petri net

pm4py.view_dfg(dfg: dict, start_activities: dict, end_activities: dict, format: str = constants.DEFAULT_FORMAT_GVIZ_VIEW, bgcolor: str = "white", max_num_edges: int = sys.maxsize, rankdir: str = constants.DEFAULT_RANKDIR_GVIZ)
pm4py.save_vis_dfg(dfg: dict, start_activities: dict, end_activities: dict, file_path: str, bgcolor: str = "white", max_num_edges: int = sys.maxsize, rankdir: str = constants.DEFAULT_RANKDIR_GVIZ, **kwargs)
Views or saves a DFG visualization to a file

pm4py.view_performance_dfg(dfg: dict, start_activities: dict, end_activities: dict, format: str = constants.DEFAULT_FORMAT_GVIZ_VIEW, aggregation_measure="mean", bgcolor: str = "white", rankdir: str = constants.DEFAULT_RANKDIR_GVIZ, serv_time: Optional[Dict[str, float]] = None)
pm4py.save_vis_performance_dfg(dfg: dict, start_activities: dict, end_activities: dict, file_path: str, aggregation_measure="mean", bgcolor: str = "white", rankdir: str = constants.DEFAULT_RANKDIR_GVIZ, serv_time: Optional[Dict[str, float]] = None, **kwargs)
Views or saves the visualization of a performance DFG

pm4py.view_process_tree(tree: ProcessTree, format: str = constants.DEFAULT_FORMAT_GVIZ_VIEW, bgcolor: str = "white", rankdir: str = constants.DEFAULT_RANKDIR_GVIZ)
pm4py.save_vis_process_tree(tree: ProcessTree, file_path: str, bgcolor: str = "white", rankdir: str = constants.DEFAULT_RANKDIR_GVIZ, **kwargs)
Views or saves the visualization of a process tree

pm4py.view_bpmn(bpmn_graph: BPMN, format: str = constants.DEFAULT_FORMAT_GVIZ_VIEW, bgcolor: str = "white", rankdir: str = constants.DEFAULT_RANKDIR_GVIZ, variant_str: str = "classic")
pm4py.save_vis_bpmn(bpmn_graph: BPMN, file_path: str, bgcolor: str = "white", rankdir: str = constants.DEFAULT_RANKDIR_GVIZ, variant_str: str = "classic", **kwargs)
Views or saves a BPMN graph

pm4py.view_heuristics_net(heu_net: HeuristicsNet, format: str = "png", bgcolor: str = "white")
pm4py.save_vis_heuristics_net(heu_net: HeuristicsNet, file_path: str, bgcolor: str = "white", **kwargs)
Views or saves the visualization of an heuristics net

pm4py.view_dotted_chart(log: Union[EventLog, pd.DataFrame], format: str = "png", attributes=None, bgcolor: str = "white", show_legend: bool = True)
pm4py.save_vis_dotted_chart(log: Union[EventLog, pd.DataFrame], file_path: str, attributes=None, bgcolor: str = "white", show_legend: bool = True, **kwargs)
Views or saves the visualization of the dotted chart. The dotted chart is a classic visualization of the events inside an event log across different dimensions. Each event of the event log is corresponding to a point. The dimensions are projected on a graph having: X axis: the values of the first dimension are represented there; Y-axis: the values of the second dimension are represented there; Color: the values of the third dimension are represented as different colors for the points of the dotted chart. The values can be either string, numeric or date values, and are managed accordingly by the dotted chart. The dotted chart can be built on different attributes. A convenient choice for the dotted chart is to visualize the distribution of cases and events over the time, with the following choices: X-axis: the timestamp of the event; Y-axis: the index of the case inside the event log; Color: the activity of the event. The aforementioned choice permits to identify visually patterns such as: Batches; Variations in the case arrival rate; Variations in the case finishing rate.

pm4py.view_sna(sna_metric: SNA, variant_str: Optional[str] = None)
pm4py.save_vis_sna(sna_metric: SNA, file_path: str, variant_str: Optional[str] = None, **kwargs)
Represents a SNA metric (in a .html file)

pm4py.view_case_duration_graph(log: Union[EventLog, pd.DataFrame], format: str = "png", activity_key="concept:name", timestamp_key="time:timestamp", case_id_key="case:concept:name")
pm4py.save_vis_case_duration_graph(log: Union[EventLog, pd.DataFrame], file_path: str, activity_key="concept:name", timestamp_key="time:timestamp", case_id_key="case:concept:name", **kwargs)
Views or saves the case duration graph

pm4py.view_events_per_time_graph(log: Union[EventLog, pd.DataFrame], format: str = "png", activity_key="concept:name", timestamp_key="time:timestamp", case_id_key="case:concept:name")
pm4py.save_vis_events_per_time_graph(log: Union[EventLog, pd.DataFrame], file_path: str, activity_key="concept:name", timestamp_key="time:timestamp", case_id_key="case:concept:name", **kwargs)
Views or saves the events per time graph

pm4py.view_performance_spectrum(log: Union[EventLog, pd.DataFrame], activities: List[str], format: str = "png", activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", bgcolor: str = "white")
pm4py.save_vis_performance_spectrum(log: Union[EventLog, pd.DataFrame], activities: List[str], file_path: str, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", bgcolor: str = "white", **kwargs)
Views or saves the performance spectrum

pm4py.view_events_distribution_graph(log: Union[EventLog, pd.DataFrame], distr_type: str = "days_week", format="png", activity_key="concept:name", timestamp_key="time:timestamp", case_id_key="case:concept:name")
pm4py.save_vis_events_distribution_graph(log: Union[EventLog, pd.DataFrame], file_path: str, distr_type: str = "days_week", activity_key="concept:name", timestamp_key="time:timestamp", case_id_key="case:concept:name", **kwargs)
Views or saves the distribution of the events. Observing the distribution of events over time permits to infer useful information about the work shifts, the working days, and the period of the year that are more or less busy.
:param distr_type: Type of distribution (default: days_week): - days_month => Gets the distribution of the events among the days of a month (from 1 to 31) - months => Gets the distribution of the events among the months (from 1 to 12) - years => Gets the distribution of the events among the years of the event log - hours => Gets the distribution of the events among the hours of a day (from 0 to 23) - days_week => Gets the distribution of the events among the days of a week (from Monday to Sunday)

pm4py.view_network_analysis(network_analysis: Dict[Tuple[str, str], Dict[str, Any]], variant: str = "frequency", format: str = constants.DEFAULT_FORMAT_GVIZ_VIEW, activity_threshold: int = 1, edge_threshold: int = 1, bgcolor: str = "white")
pm4py.save_vis_network_analysis(network_analysis: Dict[Tuple[str, str], Dict[str, Any]], file_path: str, variant: str = "frequency", activity_threshold: int = 1, edge_threshold: int = 1, bgcolor: str = "white", **kwargs)
Views or saves the network analysis.

pm4py.view_transition_system(transition_system: TransitionSystem, format: str = constants.DEFAULT_FORMAT_GVIZ_VIEW, bgcolor: str = "white")
pm4py.save_vis_transition_system(transition_system: TransitionSystem, file_path: str, bgcolor: str = "white", **kwargs)
Views or saves a transition system

pm4py.view_prefix_tree(trie: Trie, format: str = constants.DEFAULT_FORMAT_GVIZ_VIEW, bgcolor: str = "white")
pm4py.save_vis_prefix_tree(trie: Trie, file_path: str, bgcolor: str = "white", **kwargs)
Views or saves a prefix tree

pm4py.view_alignments(log: Union[EventLog, pd.DataFrame], aligned_traces: List[Dict[str, Any]], format: str = "png")
pm4py.save_vis_alignments(log: Union[EventLog, pd.DataFrame], aligned_traces: List[Dict[str, Any]], file_path: str, **kwargs)
Views or saves the alignment table as a figure

pm4py.view_footprints(footprints: Union[Tuple[Dict[str, Any], Dict[str, Any]], Dict[str, Any]], format: str = "png")
pm4py.save_vis_footprints(footprints: Union[Tuple[Dict[str, Any], Dict[str, Any]], Dict[str, Any]], file_path: str, **kwargs)
Views or saves the footprints as a figure

pm4py.view_powl(powl: POWL, format: str = constants.DEFAULT_FORMAT_GVIZ_VIEW, bgcolor: str = "white", variant_str: str = "basic")
pm4py.save_vis_powl(powl: POWL, file_path: str, bgcolor: str = "white", rankdir: str = "TB", **kwargs)
Views or saves a POWL model.

pm4py.compute_emd(language1: Dict[List[str], float], language2: Dict[List[str], float]) -> float
Computes the earth mover distance between two stochastic languages (for example, the first extracted from the log, and the second extracted from the process model.
Example:
        import pm4py

        log = pm4py.read_xes('tests/input_data/running-example.xes')
        language_log = pm4py.get_stochastic_language(log)
        print(language_log)
        net, im, fm = pm4py.read_pnml('tests/input_data/running-example.pnml')
        language_model = pm4py.get_stochastic_language(net, im, fm)
        print(language_model)
        emd_distance = pm4py.compute_emd(language_log, language_model)
        print(emd_distance)

pm4py.check_soundness(petri_net: PetriNet, initial_marking: Marking, final_marking: Marking, print_diagnostics: bool = False) -> Tuple[bool, Dict[str, Any]]
Check if a given Petri net is a sound WF-net. In the returned object, the first element is a boolean indicating if the Petri net is a sound workflow net. The second element is a set of diagnostics collected while running WOFLAN (expressed as a dictionary associating the keys [name of the diagnostics] with the corresponding diagnostics).

pm4py.cluster_log(log: Union[EventLog, EventStream, pd.DataFrame], sklearn_clusterer=None, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Generator[EventLog, None, None]
Apply clustering to the provided event log (method based on the extraction of profiles for the traces of the event log) based on a Scikit-Learn clusterer (default: K-means with two clusters)

pm4py.insert_artificial_start_end(log: Union[EventLog, pd.DataFrame], activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", artificial_start=constants.DEFAULT_ARTIFICIAL_START_ACTIVITY, artificial_end=constants.DEFAULT_ARTIFICIAL_END_ACTIVITY) -> Union[EventLog, pd.DataFrame]
Inserts the artificial start/end activities in an event log / Pandas dataframe

pm4py.insert_case_service_waiting_time(log: Union[EventLog, pd.DataFrame], service_time_column: str = "@@service_time", sojourn_time_column: str = "@@sojourn_time", waiting_time_column: str = "@@waiting_time", activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", start_timestamp_key: str = "time:timestamp") -> pd.DataFrame
Inserts the service/waiting/sojourn times of the case in the dataframe.

pm4py.insert_case_arrival_finish_rate(log: Union[EventLog, pd.DataFrame], arrival_rate_column="@@arrival_rate", finish_rate_column="@@finish_rate", activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", start_timestamp_key: str = "time:timestamp") -> pd.DataFrame
Inserts the arrival/finish rates of the case in the dataframe. The arrival rate is computed as the difference between the start time of the case and the start time of the previous case to start. The finish rate is computed as the difference between the end time of the case and the end time of the next case to end.

pm4py.check_is_workflow_net(net: PetriNet) -> bool
Checks if the input Petri net satisfies the WF-net conditions: 1. unique source place; 2. unique sink place; 3. every node is on a path from the source to the sink

pm4py.maximal_decomposition(net: PetriNet, im: Marking, fm: Marking) -> List[Tuple[PetriNet, Marking, Marking]]
Calculate the maximal decomposition of an accepting Petri net.

pm4py.simplicity_petri_net(net: PetriNet, im: Marking, fm: Marking, variant: Optional[str] = "arc_degree") -> float
Computes the simplicity metric for a given Petri net model.
The three available approaches are: Arc degree simplicity: described in the paper     Vázquez-Barreiros, Borja, Manuel Mucientes, and Manuel Lama. "ProDiGen: Mining complete, precise and minimal structure process models with a genetic algorithm." Information Sciences 294 (2015): 315-333; Extended cardoso metric: described in the paper      "Complexity Metrics for Workflow Nets" Lassen, Kristian Bisgaard, and Wil MP van der Aalst; Extended cyclomatic metric: described in the paper       "Complexity Metrics for Workflow Nets" Lassen, Kristian Bisgaard, and Wil MP van der Aalst;

pm4py.get_enabled_transitions(net: PetriNet, marking: Marking) -> Set[PetriNet.Transition]
Gets the transitions enabled in a given marking

pm4py.get_start_activities(log: Union[EventLog, pd.DataFrame], activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Dict[str, int]
Returns the start activities from a log object

pm4py.get_end_activities(log: Union[EventLog, pd.DataFrame], activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Dict[str, int]
Returns the end activities of a log

pm4py.get_event_attributes(log: Union[EventLog, pd.DataFrame]) -> List[str]
Returns the attributes at the event level of the log

pm4py.get_trace_attributes(log: Union[EventLog, pd.DataFrame]) -> List[str]
Gets the attributes at the trace level of a log object

pm4py.get_event_attribute_values(log: Union[EventLog, pd.DataFrame], attribute: str, count_once_per_case=False, case_id_key: str = "case:concept:name") -> Dict[str, int]
Returns the values for a specified (event) attribute

pm4py.get_trace_attribute_values(log: Union[EventLog, pd.DataFrame], attribute: str, case_id_key: str = "case:concept:name") -> Dict[str, int]
Returns the values for a specified trace attribute

pm4py.get_variants(log: Union[EventLog, pd.DataFrame], activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name", max_repetitions: int = sys.maxsize) -> Union[Dict[Tuple[str], List[Trace]], Dict[Tuple[str], int]]
Gets the variants from the log

pm4py.get_stochastic_language(*args, **kwargs) -> Dict[List[str], float]
Gets the stochastic language from the provided object

pm4py.get_minimum_self_distances(log: Union[EventLog, pd.DataFrame], activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Dict[str, int]
This algorithm computes the minimum self-distance for each activity observed in an event log. The self distance of a in <a> is infinity, of a in <a,a> is 0, in <a,b,a> is 1, etc. The minimum self distance is the minimal observed self distance value in the event log.

pm4py.get_minimum_self_distance_witnesses(log: Union[EventLog, pd.DataFrame], activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Dict[str, Set[str]]
This function derives the minimum self distance witnesses. The self distance of a in <a> is infinity, of a in <a,a> is 0, in <a,b,a> is 1, etc. The minimum self distance is the minimal observed self distance value in the event log. A 'witness' is an activity that witnesses the minimum self distance. For example, if the minimum self distance of activity a in some log L is 2, then, if trace <a,b,c,a> is in log L, b and c are a witness of a.

pm4py.get_case_arrival_average(log: Union[EventLog, pd.DataFrame], activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> float
Gets the average difference between the start times of two consecutive cases.

pm4py.get_rework_cases_per_activity(log: Union[EventLog, pd.DataFrame], activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Dict[str, int]
Find out for which activities of the log the rework (more than one occurrence in the trace for the activity) occurs. The output is a dictionary associating to each of the aforementioned activities the number of cases for which the rework occurred.

pm4py.get_cycle_time(log: Union[EventLog, pd.DataFrame], activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> float
Calculates the cycle time of the event log.

pm4py.get_service_time(log: Union[EventLog, pd.DataFrame], aggregation_measure: str = "mean", activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", start_timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Dict[str, float]
Gets the activities' (average/median/...) service time in the provided event log

pm4py.get_all_case_durations(log: Union[EventLog, pd.DataFrame], business_hours: bool = False, business_hour_slots=constants.DEFAULT_BUSINESS_HOUR_SLOTS, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> List[float]
Gets the durations of the cases in the event log.

pm4py.get_case_duration(log: Union[EventLog, pd.DataFrame], case_id: str, business_hours: bool = False, business_hour_slots=constants.DEFAULT_BUSINESS_HOUR_SLOTS, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: Optional[str] = None) -> float
Gets the duration of a specific case.

pm4py.get_frequent_trace_segments(log: Union[EventLog, pd.DataFrame], min_occ: int, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Counter
Get the traces (segments of activities) from an event log object using PrefixSpan. Each trace is preceded and followed by "...", reminding that the trace/segment can be preceded and followed by any other set of activities.

pm4py.get_activity_position_summary(log: Union[EventLog, pd.DataFrame], activity: str, activity_key: str = "concept:name", timestamp_key: str = "time:timestamp", case_id_key: str = "case:concept:name") -> Dict[int, int]
Given an event log, returns a dictionary which summarize the positions of the activities in the different cases of the event log. E.g., if an activity happens 1000 times in the position 1 (the second event of a case), and 500 times in the position 2 (the third event of a case), then the returned dictionary would be: {1: 1000, 2: 500}

pm4py.connectors.extract_log_outlook_mails() -> pd.DataFrame
Extracts the history of the conversations from the local instance of Microsoft Outlook running on the current computer.
CASE ID (case:concept:name) => identifier of the conversation
ACTIVITY (concept:name) => activity that is performed in the current item (send e-mail, receive e-mail, refuse meeting ...)
TIMESTAMP (time:timestamp) => timestamp of creation of the item in Outlook
RESOURCE (org:resource) => sender of the current item

pm4py.connectors.extract_log_outlook_calendar(email_user: Optional[str] = None, calendar_id: int = 9) -> pd.DataFrame
Extracts the history of the calendar events (creation, update, start, end) in a Pandas dataframe from the local Outlook instance running on the current computer.
CASE ID (case:concept:name) => identifier of the meeting
ACTIVITY (concept:name) => one between: Meeting Created, Last Change of Meeting, Meeting Started, Meeting Completed
TIMESTAMP (time:timestamp) => the timestamp of the event
case:subject => the subject of the meeting

pm4py.connectors.extract_log_windows_events() -> pd.DataFrame
Extract a process mining dataframe from all the events recorded in the Windows registry.
CASE ID (case:concept:name) => name of the computer emitting the events.
ACTIVITY (concept:name)  => concatenation of the source name of the event and the event identifier (see https://learn.microsoft.com/en-us/previous-versions/windows/desktop/eventlogprov/win32-ntlogevent)
TIMESTAMP (time:timestamp) => timestamp of generation of the event
RESOURCE (org:resource) => username involved in the event

pm4py.connectors.extract_log_chrome_history(history_db_path: Optional[str] = None) -> pd.DataFrame
Extracts a dataframe containing the navigation history of Google Chrome. Please keep Google Chrome history closed when extracting.
CASE ID (case:concept:name) => an identifier of the profile that has been extracted
ACTIVITY (concept:name) => the complete path of the website, minus the GET arguments
TIMESTAMP (time:timestamp) => the timestamp of visit
:param history_db_path: path to the history DB path of Google Chrome (default: position of the Windows folder)

pm4py.connectors.extract_log_firefox_history(history_db_path: Optional[str] = None) -> pd.DataFrame
Extracts a dataframe containing the navigation history of Mozilla Firefox. Please keep Google Chrome history closed when extracting.
CASE ID (case:concept:name) => an identifier of the profile that has been extracted
ACTIVITY (concept:name) => the complete path of the website, minus the GET arguments
TIMESTAMP (time:timestamp) => the timestamp of visit
:param history_db_path: path to the history DB path of Mozilla Firefox (default: position of the Windows folder)

pm4py.connectors.extract_log_github(owner: str = "pm4py", repo: str = "pm4py-core", auth_token: Optional[str] = None) -> pd.DataFrame
Extracts a dataframe containing the history of the issues of a Github repository. According to the API limit rate of public/registered users, only a part of the events can be returned.
:param owner: owner of the repository (e.g., pm4py)
:param repo: name of the repository (e.g., pm4py-core)
:param auth_token: authorization token

pm4py.connectors.extract_log_camunda_workflow(connection_string: str) -> pd.DataFrame
Extracts a dataframe from the Camunda workflow system. Aside from the traditional columns, the processID of the process in Camunda is returned.
:param connection_string: ODBC connection string to the Camunda database


OBJECT-CENTRIC EVENT LOGS

The Object-Centric Event Log (OCEL) 2.0 specification comes with different file formats: JSON (.json or .jsonocel), XML (.xml or .xmlocel), SQLite (.sqlite) or CSV (.csv).

In the pm4py process mining library, the OCEL class is a collection of different dataframes, containing at least the following columns:


ocel.events
 #   Column          Non-Null Count  Dtype
---  ------          --------------  -----
 0   ocel:eid        23 non-null     string
 1   ocel:timestamp  23 non-null     datetime64[ns, UTC]
 2   ocel:activity   23 non-null     string
 
ocel.objects
#   Column     Non-Null Count  Dtype
---  ------     --------------  -----
 0   ocel:oid   15 non-null     string
 1   ocel:type  15 non-null     string

ocel.relations 
 #   Column          Non-Null Count  Dtype
---  ------          --------------  -----
 0   ocel:eid        39 non-null     string
 1   ocel:activity   39 non-null     string
 2   ocel:timestamp  39 non-null     datetime64[ns, UTC]
 3   ocel:oid        39 non-null     string
 4   ocel:type       39 non-null     string
 5   ocel:qualifier  0 non-null      object

 
The 'ocel.relations' dataframe contains the many-to-many relationships between events and objects (E2O). An event can be related to different objects of different object types. The relationship can be qualified (i.e., described by a qualifier).

Moreover, there is an ocel.o2o dataframe containing the object to object (O2O) relationships. Also these relationships can be qualified.


 #   Column          Non-Null Count  Dtype
---  ------          --------------  -----
 0   ocel:oid        0 non-null      string
 1   ocel:oid_2      0 non-null      string
 2   ocel:qualifier  0 non-null      float64


In the pm4py process mining library, the following methods are available to work on object-centric event logs:

pm4py.read_ocel2(file_path: str, encoding: str = constants.DEFAULT_ENCODING) -> OCEL
Reads an OCEL2.0 event log from the disk

pm4py.write_ocel2(ocel: OCEL, file_path: str, encoding: str = constants.DEFAULT_ENCODING)
Writes an OCEL2.0 object to disk

pm4py.ocel_get_object_types(ocel: OCEL) -> List[str]
Gets the list of object types contained in the object-centric event log (e.g., ["order", "item", "delivery"]).

pm4py.ocel_get_attribute_names(ocel: OCEL) -> List[str]
Gets the list of attributes at the event and the object level of an object-centric event log (e.g. ["cost", "amount", "name"])

pm4py.ocel_flattening(ocel: OCEL, object_type: str) -> pd.DataFrame
Flattens the object-centric event log to a traditional event log with the choice of an object type. In the flattened log, the objects of a given object type are the cases, and each case contains the set of events related to the object.

pm4py.ocel_object_type_activities(ocel: OCEL) -> Dict[str, Collection[str]]
Gets the set of activities performed for each object type

pm4py.ocel_objects_ot_count(ocel: OCEL) -> Dict[str, Dict[str, int]]
Counts for each event the number of related objects per type

pm4py.ocel_temporal_summary(ocel: OCEL) -> pd.DataFrame
Returns the ``temporal summary'' from an object-centric event log. The temporal summary aggregates all the events performed in the same timestamp, and reports the list of activities and the involved objects.

pm4py.ocel_objects_summary(ocel: OCEL) -> pd.DataFrame
Gets the objects summary of an object-centric event log

pm4py.ocel_objects_interactions_summary(ocel: OCEL) -> pd.DataFrame
Gets the objects interactions summary of an object-centric event log. The objects interactions summary has a row for every combination (event, related object, other related object). Properties such as the activity of the event, and the object types of the two related objects, are included.
	
pm4py.discover_ocdfg(ocel: OCEL, business_hours=False, business_hour_slots=constants.DEFAULT_BUSINESS_HOUR_SLOTS) -> Dict[str, Any]:
Discovers an OC-DFG from an object-centric event log.

pm4py.discover_oc_petri_net(ocel: OCEL, inductive_miner_variant: str = "im", diagnostics_with_tbr: bool = False) -> Dict[str, Any]
Discovers an object-centric Petri net from the provided object-centric event log.

pm4py.discover_objects_graph(ocel: OCEL, graph_type: str = "object_interaction") -> Set[Tuple[str, str]]
Discovers an object graph from the provided object-centric event log

pm4py.ocel_o2o_enrichment(ocel: OCEL, included_graphs: Optional[Collection[str]] = None) -> OCEL
Inserts the information inferred from the graph computations (pm4py.discover_objects_graph) in the list of O2O relations of the OCEL.

pm4py.ocel_e2o_lifecycle_enrichment(ocel: OCEL) -> OCEL
Inserts lifecycle-based information (when an object is created/terminated or other types of relations) in the list of E2O relations of the OCEL

pm4py.sample_ocel_objects(ocel: OCEL, num_objects: int) -> OCEL
Given an object-centric event log, returns a sampled event log with a subset of the objects that is chosen in a random way. Only the events related to at least one of these objects are filtered from the event log. As a note, the relationships between the different objects are probably going to be ruined by this sampling.

pm4py.sample_ocel_connected_components(ocel: OCEL, connected_components: int = 1, max_num_events_per_cc: int = sys.maxsize, max_num_objects_per_cc: int = sys.maxsize, max_num_e2o_relations_per_cc: int = sys.maxsize) -> OCEL
Given an object-centric event log, returns a sampled event log with a subset of the executions. The number of considered connected components need to be specified by the user.

pm4py.ocel_drop_duplicates(ocel: OCEL) -> OCEL
Drop relations between events and objects happening at the same time, with the same activity, to the same object identifier. This ends up cleaning the OCEL from duplicate events.

pm4py.ocel_merge_duplicates(ocel: OCEL, have_common_object: Optional[bool]=False) -> OCEL
Merge events in the OCEL that happen with the same activity at the same timestamp

pm4py.ocel_sort_by_additional_column(ocel: OCEL, additional_column: str, primary_column: str = "ocel:timestamp") -> OCEL
Sorts the OCEL not only based on the timestamp column and the index, but using an additional sorting column that further determines the order of the events happening at the same timestamp.

pm4py.ocel_add_index_based_timedelta(ocel: OCEL) -> OCEL
Adds a small time-delta to the timestamp column based on the current index of the event. This ensures the correct ordering of the events in any object-centric process mining solution.

pm4py.cluster_equivalent_ocel(ocel: OCEL, object_type: str, max_objs: int = sys.maxsize) -> Dict[str, Collection[OCEL]]
Perform a clustering of the object-centric event log, based on the 'executions' of a single object type. Equivalent 'executions' are grouped in the output dictionary.

pm4py.read_ocel2(file_path: str, encoding: str = constants.DEFAULT_ENCODING) -> OCEL
Reads an OCEL2.0 event log

pm4py.write_ocel2(ocel: OCEL, file_path: str, encoding: str = constants.DEFAULT_ENCODING)
Writes an OCEL2.0 object to disk

pm4py.filter_ocel_event_attribute(ocel: OCEL, attribute_key: str, attribute_values: Collection[Any], positive: bool = True) -> OCEL
Filters the object-centric event log on the provided event attributes values

pm4py.filter_ocel_object_attribute(ocel: OCEL, attribute_key: str, attribute_values: Collection[Any], positive: bool = True) -> OCEL
Filters the object-centric event log on the provided object attributes values

pm4py.filter_ocel_object_types_allowed_activities(ocel: OCEL, correspondence_dict: Dict[str, Collection[str]]) -> OCEL
Filters an object-centric event log keeping only the specified object types with the specified activity set (filters out the rest).

pm4py.filter_ocel_object_per_type_count(ocel: OCEL, min_num_obj_type: Dict[str, int]) -> OCEL
Filters the events of the object-centric logs which are related to at least the specified amount of objects per type.

pm4py.filter_ocel_start_events_per_object_type(ocel: OCEL, object_type: str) -> OCEL
Filters the events in which a new object for the given object type is spawn. (E.g. an event with activity "Create Order" might spawn new orders).

pm4py.filter_ocel_end_events_per_object_type(ocel: OCEL, object_type: str) -> OCEL
Filters the events in which an object for the given object type terminates its lifecycle. (E.g. an event with activity "Pay Order" might terminate an order).

pm4py.filter_ocel_events_timestamp(ocel: OCEL, min_timest: Union[datetime.datetime, str], max_timest: Union[datetime.datetime, str], timestamp_key: str = "ocel:timestamp") -> OCEL
Filters the object-centric event log keeping events in the provided timestamp range

pm4py.filter_ocel_object_types(ocel: OCEL, obj_types: Collection[str], positive: bool = True, level: int = 1) -> OCEL
Filters the object types of an object-centric event log.

pm4py.filter_ocel_objects(ocel: OCEL, object_identifiers: Collection[str], positive: bool = True, level: int = 1) -> OCEL
Filters the object identifiers of an object-centric event log.

pm4py.filter_ocel_events(ocel: OCEL, event_identifiers: Collection[str], positive: bool = True) -> OCEL
Filters the event identifiers of an object-centric event log.

pm4py.filter_ocel_cc_object(ocel: OCEL, object_id: str, conn_comp: Optional[List[List[str]]] = None, return_conn_comp: bool = False) -> Union[OCEL, Tuple[OCEL, List[List[str]]]]
Returns the connected component of the object-centric event log to which the object with the provided identifier belongs.

pm4py.filter_ocel_cc_length(ocel: OCEL, min_cc_length: int, max_cc_length: int) -> OCEL
Keeps only the objects in an OCEL belonging to a connected component with a length falling in a specified range

pm4py.filter_ocel_cc_otype(ocel: OCEL, otype: str, positive: bool = True) -> OCEL
Filters the objects belonging to the connected components having at least an object of the provided object type.

pm4py.filter_ocel_cc_activity(ocel: OCEL, activity: str) -> OCEL
Filters the objects belonging to the connected components having at least an event with the provided activity.

pm4py.extract_ocel_features(ocel: OCEL, obj_type: str, enable_object_lifecycle_paths: bool = True, enable_object_work_in_progress: bool = False, object_str_attributes: Optional[Collection[str]] = None, object_num_attributes: Optional[Collection[str]] = None, include_obj_id: bool = False, debug: bool = False) -> pd.DataFrame
Extracts from an object-centric event log a set of features (returned as dataframe) computed on the OCEL for the objects of a given object type.

pm4py.convert_ocel_to_networkx(ocel: OCEL, variant: str = "ocel_to_nx") -> nx.DiGraph
Converts an OCEL to a NetworkX DiGraph object.

pm4py.convert_log_to_ocel(log: Union[EventLog, EventStream, pd.DataFrame], activity_column: str = "concept:name", timestamp_column: str = "time:timestamp", object_types: Optional[Collection[str]] = None, obj_separator: str = " AND ", additional_event_attributes: Optional[Collection[str]] = None, additional_object_attributes: Optional[Dict[str, Collection[str]]] = None) -> OCEL
Converts an event log to an object-centric event log with one or more than one object types.

pm4py.view_ocdfg(ocdfg: Dict[str, Any], annotation: str = "frequency", act_metric: str = "events", edge_metric="event_couples", act_threshold: int = 0, edge_threshold: int = 0, performance_aggregation: str = "mean", format: str = constants.DEFAULT_FORMAT_GVIZ_VIEW, bgcolor: str = "white", rankdir: str = constants.DEFAULT_RANKDIR_GVIZ)
pm4py.save_vis_ocdfg(ocdfg: Dict[str, Any], file_path: str, annotation: str = "frequency", act_metric: str = "events", edge_metric="event_couples", act_threshold: int = 0, edge_threshold: int = 0, performance_aggregation: str = "mean", bgcolor: str = "white", rankdir: str = constants.DEFAULT_RANKDIR_GVIZ, **kwargs)
Views or saves an OC-DFG (object-centric directly-follows graph) with the provided configuration.
:param ocdfg: Object-centric directly-follows graph
:param annotation: The annotation to use for the visualization. Values: - "frequency": frequency annotation - "performance": performance annotation
:param act_metric: The metric to use for the activities. Available values: - "events" => number of events (default) - "unique_objects" => number of unique objects - "total_objects" => number of total objects
:param edge_metric: The metric to use for the edges. Available values: - "event_couples" => number of event couples (default) - "unique_objects" => number of unique objects - "total_objects" => number of total objects
:param act_threshold: The threshold to apply on the activities frequency (default: 0). Only activities having a frequency >= than this are kept in the graph.
:param edge_threshold: The threshold to apply on the edges frequency (default 0). Only edges having a frequency >= than this are kept in the graph.
:param performance_aggregation: The aggregation measure to use for the performance: mean, median, min, max, sum
:param format: The format of the output visualization (if html is provided, GraphvizJS is used to render the visualization in an HTML page)
:param bgcolor: Background color of the visualization (default: white)
:param rankdir: sets the direction of the graph ("LR" for left-to-right; "TB" for top-to-bottom)

pm4py.view_ocpn(ocpn: Dict[str, Any], format: str = constants.DEFAULT_FORMAT_GVIZ_VIEW, bgcolor: str = "white", rankdir: str = constants.DEFAULT_RANKDIR_GVIZ)
pm4py.save_vis_ocpn(ocpn: Dict[str, Any], file_path: str, bgcolor: str = "white", rankdir: str = constants.DEFAULT_RANKDIR_GVIZ, **kwargs)
Saves the visualization of the object-centric Petri net into a file

pm4py.view_object_graph(ocel: OCEL, graph: Set[Tuple[str, str]], format: str = constants.DEFAULT_FORMAT_GVIZ_VIEW, bgcolor: str = "white", rankdir: str = constants.DEFAULT_RANKDIR_GVIZ)
pm4py.save_vis_object_graph(ocel: OCEL, graph: Set[Tuple[str, str]], file_path: str, bgcolor: str = "white", rankdir: str = constants.DEFAULT_RANKDIR_GVIZ, **kwargs)
Saves the visualization of an object graph

pm4py.connectors.extract_ocel_outlook_mails() -> OCEL
Extracts the history of the conversations from the local instance of Microsoft Outlook running on the current computer as an object-centric event log.
ACTIVITY (ocel:activity) => activity that is performed in the current item (send e-mail, receive e-mail, refuse meeting ...)
TIMESTAMP (ocel:timestamp) => timestamp of creation of the item in Outlook
Object types:
- org:resource => the sender of the mail
- recipients => the list of recipients of the mail
- topic => the topic of the discussion

pm4py.connectors.extract_ocel_outlook_calendar(email_user: Optional[str] = None, calendar_id: int = 9) -> OCEL
Extracts the history of the calendar events (creation, update, start, end) as an object-centric event log from the local Outlook instance running on the current computer.
ACTIVITY (ocel:activity) => one between: Meeting Created, Last Change of Meeting, Meeting Started, Meeting Completed
TIMESTAMP (ocel:timestamp) => the timestamp of the event
Object types:
- case:concept:name => identifier of the meeting
- case:subject => the subject of the meeting

pm4py.connectors.extract_ocel_windows_events() -> OCEL
Extract a process mining dataframe from all the events recorded in the Windows registry as an object-centric event log.
ACTIVITY (concept:name)  => concatenation of the source name of the event and the event identifier
			(see https://learn.microsoft.com/en-us/previous-versions/windows/desktop/eventlogprov/win32-ntlogevent)
TIMESTAMP (time:timestamp) => timestamp of generation of the event
Object types:
- categoryString: translation of the subcategory. The translation is source-specific.
- computerName: name of the computer that generated this event.
- eventIdentifier: identifier of the event. This is specific to the source that generated the event log entry.
- eventType: 1=Error; 2=Warning; 3=Information; 4=Security Audit Success;5=Security Audit Failure;
- sourceName: name of the source (application, service, driver, or subsystem) that generated the entry.
- user: user name of the logged-on user when the event occurred. If the user name cannot be determined, this will be NULL.

pm4py.connectors.extract_ocel_chrome_history(history_db_path: Optional[str] = None) -> OCEL
Extracts an object-centric event log containing the navigation history of Google Chrome.
Please keep Google Chrome history closed when extracting.
ACTIVITY (ocel:activity) => the complete path of the website, minus the GET arguments
TIMESTAMP (ocel:timestamp) => the timestamp of visit
Object Types:
- case:concept:name : the profile of Chrome that is used to visit the site
- complete_url: the complete URL of the website
- url_wo_parameters: complete URL minus the part after ?
- domain: the domain of the website that is visited

pm4py.connectors.extract_ocel_firefox_history(history_db_path: Optional[str] = None) -> OCEL
Extracts an object-centric event log containing the navigation history of Mozilla Firefox. Please keep Mozilla Firefox history closed when extracting.
ACTIVITY (ocel:activity) => the complete path of the website, minus the GET arguments
TIMESTAMP (ocel:timestamp) => the timestamp of visit
Object Types:
- case:concept:name : the profile of Firefox that is used to visit the site
- complete_url: the complete URL of the website
- url_wo_parameters: complete URL minus the part after ?
- domain: the domain of the website that is visited

pm4py.connectors.extract_ocel_github(owner: str = "pm4py", repo: str = "pm4py-core", auth_token: Optional[str] = None) -> OCEL
Extracts a dataframe containing the history of the issues of a Github repository.
According to the API limit rate of public/registered users, only a part of the events
can be returned.
ACTIVITY (ocel:activity) => the event (created, commented, closed, subscribed ...)
TIMESTAMP (ocel:timestamp) => the timestamp of execution of the event
Object types:
- case:concept:name => the URL of the events related to the issue
- org:resource => the involved resource
- case:repo => the repository in which the issue is created

pm4py.connectors.extract_ocel_camunda_workflow(connection_string: str) -> OCEL
Extracts an object-centric event log from the Camunda workflow system.
